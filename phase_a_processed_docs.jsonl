{"id": "corpus_txt__6d952a1f", "title": "TXT corpus: kb_modiji.txt", "source_type": "file", "source": "/root/AI/data/corpus/txt/kb_modiji.txt", "text": "# HISTORY OF NARENDRA MODI\n\n## Early Life and Background\n\n**Birth and Family:**\n- Born: September 17, 1950, in Vadnagar, Gujarat, India\n- Parents: Damodardas Mulchand Modi (father) and Hiraben Modi (mother)\n- Family Background: Born into a lower-middle-class family of the Ghanchi-Teli (oil-presser) community\n- Siblings: Third of six children\n\n**Childhood:**\n- Helped his father run a tea stall at the Vadnagar railway station\n- Described as an average student who was more interested in theater and debates\n- Early interest in politics and public speaking\n\n## Education\n\n- Completed higher secondary education in Vadnagar\n- Bachelor's degree in Political Science from Delhi University (1978) through distance learning\n- Master's degree in Political Science from Gujarat University (1983)\n- Known to have been influenced by nationalist literature and ideology during his formative years\n\n## Early Political Career\n\n**RSS Involvement:**\n- Joined the Rashtriya Swayamsevak Sangh (RSS) at age 8\n- Became a full-time RSS worker (pracharak) in 1971\n- Served the RSS in various capacities during the 1970s and 1980s\n\n**Entry into BJP:**\n- Joined the Bharatiya Janata Party (BJP) in 1987\n- Quickly rose through party ranks due to organizational skills\n- Became General Secretary of BJP Gujarat in 1988\n- National Secretary of BJP in 1995\n\n## Chief Minister of Gujarat (2001-2014)\n\n**First Term (2001-2002):**\n- Became Chief Minister of Gujarat on October 7, 2001\n- Initial appointment came after the resignation of Keshubhai Patel\n\n**2002 Gujarat Riots:**\n- Major communal riots occurred in February-March 2002\n- Riots resulted in significant loss of life, primarily among Muslims\n- Modi's handling of the riots became controversial and subject to extensive scrutiny\n- Supreme Court-appointed Special Investigation Team (SIT) investigated allegations\n- In 2012, SIT gave a clean chit to Modi\n- Supreme Court upheld this decision in 2013\n\n**Subsequent Terms:**\n- Won elections in 2002, 2007, and 2012\n- Served as Chief Minister for nearly 13 years\n- Focused on economic development and infrastructure\n- Promoted Gujarat as a business-friendly destination\n- Launched initiatives like Vibrant Gujarat Summit\n- Known for emphasizing governance and development model\n\n**Economic Development:**\n- Gujarat experienced significant industrial growth during his tenure\n- Attracted domestic and international investment\n- Improved infrastructure including roads, ports, and power supply\n- Agricultural development programs\n- Critics pointed to concerns about inclusive growth and social development indicators\n\n## Rise to National Politics\n\n**2014 General Elections:**\n- Named BJP's Prime Ministerial candidate in September 2013\n- Led aggressive campaign focused on development and governance\n- Used social media and technology extensively\n- Campaign slogan: \"Achhe Din Aane Wale Hain\" (Good Days Are Coming)\n- BJP won historic mandate with 282 seats (first party to win majority since 1984)\n- Became Prime Minister on May 26, 2014\n\n## Prime Minister of India (2014-Present)\n\n**First Term (2014-2019):**\n\n*Major Initiatives:*\n- Swachh Bharat Abhiyan (Clean India Mission)\n- Make in India campaign\n- Digital India initiative\n- Jan Dhan Yojana (financial inclusion)\n- Skill India program\n- Pradhan Mantri Ujjwala Yojana (LPG connections)\n- Goods and Services Tax (GST) implementation (2017)\n- Demonetization (November 2016)\n\n*Foreign Policy:*\n- Proactive foreign policy with visits to numerous countries\n- Emphasis on \"Neighborhood First\" policy\n- Strong focus on diaspora engagement\n- Enhanced strategic partnerships globally\n\n**Second Term (2019-2024):**\n- Won re-election with increased majority (303 seats)\n- Sworn in on May 30, 2019\n\n*Major Developments:*\n- Abrogation of Article 370 (special status of Jammu and Kashmir) in August 2019\n- Citizenship Amendment Act (2019)\n- COVID-19 pandemic management (2020-2022)\n- Vaccine development and distribution (Covaxin, Covishield)\n- Ayushman Bharat health insurance scheme expansion\n- Housing for All initiatives\n- Infrastructure development focus\n\n**Third Term (2024-Present):**\n- Won 2024 general elections\n- BJP did not secure absolute majority on its own but formed government with NDA alliance\n- Currently serving as Prime Minister\n\n## Key Policies and Programs\n\n**Economic Initiatives:**\n- Production Linked Incentive (PLI) Scheme\n- Atmanirbhar Bharat (Self-Reliant India)\n- Startup India\n- Mudra Yojana for small businesses\n- National Infrastructure Pipeline\n\n**Social Welfare:**\n- Beti Bachao Beti Padhao (Save and Educate Girl Child)\n- PM-KISAN (direct income support to farmers)\n- Ayushman Bharat (health insurance)\n- Housing schemes\n- Rural development programs\n\n**Technology and Innovation:**\n- Digital payments and UPI expansion\n- Aadhaar-based service delivery\n- Smart Cities Mission\n- Space program advancement\n\n**Environmental Initiatives:**\n- International Solar Alliance\n- Climate change commitments\n- Renewable energy targets\n- Electric vehicle promotion\n\n## Leadership Style\n\n- Known for centralized decision-making\n- Strong emphasis on personal branding and communication\n- Active on social media with massive following\n- Regular radio program \"Mann Ki Baat\" (since 2014)\n- Emphasis on direct communication with citizens\n- Focus on nationalism and cultural identity\n\n## Recognition and Awards\n\n- Time Magazine's Person of the Year (reader's poll) multiple times\n- Various international awards and honors\n- Honorary doctorates from multiple universities\n- Global leadership recognition\n\n## Controversies and Criticism\n\n- 2002 Gujarat riots and his role\n- Concerns about secular fabric and minority rights\n- Press freedom and civil liberties debates\n- Economic policies and their impact\n- Handling of farmers' protests\n- COVID-19 pandemic management criticism\n- Demonetization impact debates\n\n## Personal Life\n\n- Married to Jashodaben Chimanlal Modi (estranged marriage from youth)\n- Known for simple lifestyle and long working hours\n- Vegetarian and teetotaler\n- Practices yoga and meditation\n- Interests in photography and writing\n\n## Legacy and Impact\n\nNarendra Modi remains one of the most influential and polarizing figures in contemporary Indian politics. His tenure has been marked by significant policy initiatives, economic reforms, and a distinctive approach to governance and foreign policy. Supporters credit him with strong leadership, economic development, and enhanced global standing, while critics raise concerns about democratic values, inclusive growth, and social harmony.\n\n---\n\n*Note: This document presents a factual overview of Narendra Modi's history and career. He continues to serve as Prime Minister of India, and his legacy remains a subject of ongoing debate and assessment.*", "meta": {"format": "txt", "tags": ["txt"], "source_id": "corpus_txt__6d952a1f", "source_type": "file", "file_path_relative": "data/corpus/txt/kb_modiji.txt", "file_path_absolute": "/root/AI/data/corpus/txt/kb_modiji.txt", "loader_options_used": {}}}
{"id": "corpus_txt__5ab8571a", "title": "TXT corpus: quantum.txt", "source_type": "file", "source": "/root/AI/data/corpus/txt/quantum.txt", "text": "Quantum computing represents a revolutionary approach to processing information. Unlike classical computers that use bits representing 0 or 1, quantum computers use qubits that can exist in superposition, representing both states simultaneously. This property, combined with quantum entanglement, allows quantum computers to solve certain problems exponentially faster than classical computers. Companies like IBM, Google, and startups are racing to build practical quantum computers. The technology promises breakthroughs in cryptography, drug discovery, financial modeling, and optimization problems that are currently intractable for traditional computers.\n\nThe Mediterranean diet has long been celebrated as one of the healthiest eating patterns in the world. Originating from countries bordering the Mediterranean Sea, this diet emphasizes whole grains, fresh fruits and vegetables, olive oil, fish, and moderate amounts of dairy and wine. Research consistently shows that people following this diet have lower rates of heart disease, diabetes, and certain cancers. The anti-inflammatory properties of olive oil, combined with omega-3 fatty acids from fish and antioxidants from produce, create a powerful nutritional profile. Beyond physical health, the Mediterranean lifestyle promotes social eating and mindful enjoyment of meals.", "meta": {"format": "txt", "tags": ["txt"], "source_id": "corpus_txt__5ab8571a", "source_type": "file", "file_path_relative": "data/corpus/txt/quantum.txt", "file_path_absolute": "/root/AI/data/corpus/txt/quantum.txt", "loader_options_used": {}}}
{"id": "dspy_rag_tutorial", "title": "DSPy RAG Tutorial", "source_type": "url", "source": "https://dspy.ai/tutorials/rag/", "text": "Skip to content Tutorial: Retrieval-Augmented Generation (RAG) ¶ Let's walk through a quick example of basic question answering with and without retrieval-augmented generation (RAG) in DSPy. Specifically, let's build a system for answering Tech questions , e.g. about Linux or iPhone apps. Install the latest DSPy via pip install -U dspy and follow along. If you're looking instead for a conceptual overview of DSPy, this recent lecture is a good place to start. You also need to run pip install datasets . Configuring the DSPy environment. ¶ Let's tell DSPy that we will use OpenAI's gpt-4o-mini in our modules. To authenticate, DSPy will look into your OPENAI_API_KEY . You can easily swap this out for other providers or local models . Recommended: Set up MLflow Tracing to understand what's happening under the hood. MLflow DSPy Integration ¶ MLflow is an LLMOps tool that natively integrates with DSPy and offer explainability and experiment tracking. In this tutorial, you can use MLflow to visualize prompts and optimization progress as traces to understand the DSPy's behavior better. You can set up MLflow easily by following the four steps below. Install MLflow %pip install mlflow> = 2 .20 Start MLflow UI in a separate terminal mlflow ui --port 5000 Connect the notebook to MLflow import mlflow mlflow . set_tracking_uri ( \"http://localhost:5000\" ) mlflow . set_experiment ( \"DSPy\" ) Enabling tracing. mlflow . dspy . autolog () Once you have completed the steps above, you can see traces for each program execution on the notebook. They provide great visibility into the model's behavior and helps you understand the DSPy's concepts better throughout the tutorial. To kearn more about the integration, visit MLflow DSPy Documentation as well. In [1]: Copied! import dspy lm = dspy . LM ( 'openai/gpt-4o-mini' ) dspy . configure ( lm = lm ) import dspy lm = dspy.LM('openai/gpt-4o-mini')\ndspy.configure(lm=lm) Exploring some basic DSPy Modules. ¶ You can always prompt the LM directly via lm(prompt=\"prompt\") or lm(messages=[...]) . However, DSPy gives you Modules as a better way to define your LM functions. The simplest module is dspy.Predict . It takes a DSPy Signature , i.e. a structured input/output schema, and gives you back a callable function for the behavior you specified. Let's use the \"in-line\" notation for signatures to declare a module that takes a question (of type str ) as input and produces a response as an output. In [2]: Copied! qa = dspy . Predict ( 'question: str -> response: str' ) response = qa ( question = \"what are high memory and low memory on linux?\" ) print ( response . response ) qa = dspy.Predict('question: str -> response: str')\nresponse = qa(question=\"what are high memory and low memory on linux?\") print(response.response) In Linux, \"high memory\" and \"low memory\" refer to different regions of the system's memory address space, particularly in the context of 32-bit architectures. - **Low Memory**: This typically refers to the memory that is directly accessible by the kernel. In a 32-bit system, this is usually the first 896 MB of RAM (from 0 to 896 MB). The kernel can directly map this memory, making it faster for the kernel to access and manage. Low memory is used for kernel data structures and for user processes that require direct access to memory. - **High Memory**: This refers to the memory above the low memory limit, which is not directly accessible by the kernel in a 32-bit system. This area is typically above 896 MB. The kernel cannot directly access this memory without using special mechanisms, such as mapping it into the kernel's address space when needed. High memory is used for user processes that require more memory than what is available in low memory. In summary, low memory is directly accessible by the kernel, while high memory requires additional steps for the kernel to access it, especially in 32-bit systems. In 64-bit systems, this distinction is less significant as the kernel can address a much larger memory space directly. Notice how the variable names we specified in the signature defined our input and output argument names and their role. Now, what did DSPy do to build this qa module? Nothing fancy in this example, yet. The module passed your signature, LM, and inputs to an Adapter, which is a layer that handles structuring the inputs and parsing structured outputs to fit your signature. Let's see it directly. You can inspect the n last prompts sent by DSPy easily. Alternatively, if you enabled MLflow Tracing above, you can see the full LLM interactions for each program execution in a tree view. In [3]: Copied! dspy . inspect_history ( n = 1 ) dspy.inspect_history(n=1) [2024-11-23T23:16:35.966534] System message: Your input fields are:\n1. `question` (str) Your output fields are:\n1. `response` (str) All interactions will be structured in the following way, with the appropriate values filled in. [[ ## question ## ]]\n{question} [[ ## response ## ]]\n{response} [[ ## completed ## ]] In adhering to this structure, your objective is: Given the fields `question`, produce the fields `response`. User message: [[ ## question ## ]]\nwhat are high memory and low memory on linux? Respond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`. Response: [[ ## response ## ]]\nIn Linux, \"high memory\" and \"low memory\" refer to different regions of the system's memory address space, particularly in the context of 32-bit architectures. - **Low Memory**: This typically refers to the memory that is directly accessible by the kernel. In a 32-bit system, this is usually the first 896 MB of RAM (from 0 to 896 MB). The kernel can directly map this memory, making it faster for the kernel to access and manage. Low memory is used for kernel data structures and for user processes that require direct access to memory. - **High Memory**: This refers to the memory above the low memory limit, which is not directly accessible by the kernel in a 32-bit system. This area is typically above 896 MB. The kernel cannot directly access this memory without using special mechanisms, such as mapping it into the kernel's address space when needed. High memory is used for user processes that require more memory than what is available in low memory. In summary, low memory is directly accessible by the kernel, while high memory requires additional steps for the kernel to access it, especially in 32-bit systems. In 64-bit systems, this distinction is less significant as the kernel can address a much larger memory space directly. [[ ## completed ## ]] DSPy has various built-in modules, e.g. dspy.ChainOfThought , dspy.ProgramOfThought , and dspy.ReAct . These are interchangeable with basic dspy.Predict : they take your signature, which is specific to your task, and they apply general-purpose prompting techniques and inference-time strategies to it. For example, dspy.ChainOfThought is an easy way to elicit reasoning out of your LM before it commits to the outputs requested in your signature. In the example below, we'll omit str types (as the default type is string). You should feel free to experiment with other fields and types, e.g. try topics: list[str] or is_realistic: bool . In [4]: Copied! cot = dspy . ChainOfThought ( 'question -> response' ) cot ( question = \"should curly braces appear on their own line?\" ) cot = dspy.ChainOfThought('question -> response')\ncot(question=\"should curly braces appear on their own line?\") Out[4]: Prediction( reasoning='The placement of curly braces on their own line depends on the coding style and conventions being followed. In some programming languages and style guides, such as the Allman style, curly braces are placed on their own line to enhance readability. In contrast, other styles, like K&R style, place the opening brace on the same line as the control statement. Ultimately, it is a matter of personal or team preference, and consistency within a project is key.', response='Curly braces can appear on their own line depending on the coding style you are following. If you prefer a style that enhances readability, such as the Allman style, then yes, they should be on their own line. However, if you are following a different style, like K&R, they may not need to be. Consistency is important, so choose a style and stick with it.'\n) Interestingly, asking for reasoning can make the output response shorter in this case. Is this a good thing or a bad thing? It depends on what you need: there's no free lunch, but DSPy gives you the tools to experiment with different strategies extremely quickly. By the way, dspy.ChainOfThought is implemented in DSPy, using dspy.Predict . This is a good place to dspy.inspect_history if you're curious. Using DSPy well involves evaluation and iterative development. ¶ You already know a lot about DSPy at this point. If all you want is quick scripting, this much of DSPy already enables a lot. Sprinkling DSPy signatures and modules into your Python control flow is a pretty ergonomic way to just get stuff done with LMs. That said, you're likely here because you want to build a high-quality system and improve it over time. The way to do that in DSPy is to iterate fast by evaluating the quality of your system and using DSPy's powerful tools, e.g. Optimizers. Manipulating Examples in DSPy. ¶ To measure the quality of your DSPy system, you need (1) a bunch of input values, like question s for example, and (2) a metric that can score the quality of an output from your system. Metrics vary widely. Some metrics need ground-truth labels of ideal outputs, e.g. for classification or question answering. Other metrics are self-supervised, e.g. checking faithfulness or lack of hallucination, perhaps using a DSPy program as a judge of these qualities. Let's load a dataset of questions and their (pretty long) gold answers. Since we started this notebook with the goal of building a system for answering Tech questions , we obtained a bunch of StackExchange-based questions and their correct answers from the RAG-QA Arena dataset. In [5]: Copied! import orjson from dspy.utils import download # Download question--answer pairs from the RAG-QA Arena \"Tech\" dataset. download ( \"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_examples.jsonl\" ) with open ( \"ragqa_arena_tech_examples.jsonl\" ) as f : data = [ orjson . loads ( line ) for line in f ] import orjson\nfrom dspy.utils import download # Download question--answer pairs from the RAG-QA Arena \"Tech\" dataset.\ndownload(\"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_examples.jsonl\") with open(\"ragqa_arena_tech_examples.jsonl\") as f: data = [orjson.loads(line) for line in f] In [6]: Copied! # Inspect one datapoint. data [ 0 ] # Inspect one datapoint.\ndata[0] Out[6]: {'question': 'why igp is used in mpls?', 'response': \"An IGP exchanges routing prefixes between gateways/routers. \\nWithout a routing protocol, you'd have to configure each route on every router and you'd have no dynamic updates when routes change because of link failures. \\nFuthermore, within an MPLS network, an IGP is vital for advertising the internal topology and ensuring connectivity for MP-BGP inside the network.\", 'gold_doc_ids': [2822, 2823]} Given a simple dict like this, let's create a list of dspy.Example s, which is the datatype that carries training (or test) datapoints in DSPy. When you build a dspy.Example , you should generally specify .with_inputs(\"field1\", \"field2\", ...) to indicate which fields are inputs. The other fields are treated as labels or metadata. In [7]: Copied! data = [ dspy . Example ( ** d ) . with_inputs ( 'question' ) for d in data ] # Let's pick an `example` here from the data. example = data [ 2 ] example data = [dspy.Example(**d).with_inputs('question') for d in data] # Let's pick an `example` here from the data.\nexample = data[2]\nexample Out[7]: Example({'question': 'why are my text messages coming up as maybe?', 'response': 'This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \\n\\nHowever, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.', 'gold_doc_ids': [3956, 3957, 8034]}) (input_keys={'question'}) Now, let's divide the data into: Training (and with it Validation) set: These are the splits you typically give to DSPy optimizers. Optimizers typically learn directly from the training examples and check their progress using the validation examples. It's good to have 30--300 examples for training and validation each. For prompt optimizers in particular, it's often better to pass more validation than training. Below, we'll use 200 in total. MIPROv2 will split them into 20% training and 80% validation if you don't pass a valset. Development and Test sets: The rest, typically on the order of 30--1000, can be used for: development (i.e., you can inspect them as you iterate on your system) and testing (final held-out evaluation). In [8]: Copied! import random random . Random ( 0 ) . shuffle ( data ) trainset , devset , testset = data [: 200 ], data [ 200 : 500 ], data [ 500 : 1000 ] len ( trainset ), len ( devset ), len ( testset ) import random random.Random(0).shuffle(data)\ntrainset, devset, testset = data[:200], data[200:500], data[500:1000] len(trainset), len(devset), len(testset) Out[8]: (200, 300, 500) Evaluation in DSPy. ¶ What kind of metric can suit our question-answering task? There are many choices, but since the answers are long, we may ask: How well does the system response cover all key facts in the gold response? And the other way around, how well is the system response not saying things that aren't in the gold response? That metric is essentially a semantic F1 , so let's load a SemanticF1 metric from DSPy. This metric is actually implemented as a very simple DSPy module using whatever LM we're working with. In [9]: Copied! from dspy.evaluate import SemanticF1 # Instantiate the metric. metric = SemanticF1 ( decompositional = True ) # Produce a prediction from our `cot` module, using the `example` above as input. pred = cot ( ** example . inputs ()) # Compute the metric score for the prediction. score = metric ( example , pred ) print ( f \"Question: \\t { example . question } \\n \" ) print ( f \"Gold Response: \\t { example . response } \\n \" ) print ( f \"Predicted Response: \\t { pred . response } \\n \" ) print ( f \"Semantic F1 Score: { score : .2f } \" ) from dspy.evaluate import SemanticF1 # Instantiate the metric.\nmetric = SemanticF1(decompositional=True) # Produce a prediction from our `cot` module, using the `example` above as input.\npred = cot(**example.inputs()) # Compute the metric score for the prediction.\nscore = metric(example, pred) print(f\"Question: \\t {example.question}\\n\")\nprint(f\"Gold Response: \\t {example.response}\\n\")\nprint(f\"Predicted Response: \\t {pred.response}\\n\")\nprint(f\"Semantic F1 Score: {score:.2f}\") Question: why are my text messages coming up as maybe? Gold Response: This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". However, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled. Predicted Response: Your text messages are showing up as \"maybe\" because your messaging app is uncertain about the sender's identity. This typically occurs when the sender's number is not saved in your contacts or if the message is from an unknown number. To resolve this, you can save the contact in your address book or check the message settings in your app. Semantic F1 Score: 0.33 The final DSPy module call above actually happens inside metric . You might be curious how it measured the semantic F1 for this example. In [10]: Copied! dspy . inspect_history ( n = 1 ) dspy.inspect_history(n=1) [2024-11-23T23:16:36.149518] System message: Your input fields are:\n1. `question` (str)\n2. `ground_truth` (str)\n3. `system_response` (str) Your output fields are:\n1. `reasoning` (str)\n2. `ground_truth_key_ideas` (str): enumeration of key ideas in the ground truth\n3. `system_response_key_ideas` (str): enumeration of key ideas in the system response\n4. `discussion` (str): discussion of the overlap between ground truth and system response\n5. `recall` (float): fraction (out of 1.0) of ground truth covered by the system response\n6. `precision` (float): fraction (out of 1.0) of system response covered by the ground truth All interactions will be structured in the following way, with the appropriate values filled in. [[ ## question ## ]]\n{question} [[ ## ground_truth ## ]]\n{ground_truth} [[ ## system_response ## ]]\n{system_response} [[ ## reasoning ## ]]\n{reasoning} [[ ## ground_truth_key_ideas ## ]]\n{ground_truth_key_ideas} [[ ## system_response_key_ideas ## ]]\n{system_response_key_ideas} [[ ## discussion ## ]]\n{discussion} [[ ## recall ## ]]\n{recall} # note: the value you produce must be a single float value [[ ## precision ## ]]\n{precision} # note: the value you produce must be a single float value [[ ## completed ## ]] In adhering to this structure, your objective is: Compare a system's response to the ground truth to compute recall and precision of key ideas. You will first enumerate key ideas in each response, discuss their overlap, and then report recall and precision. User message: [[ ## question ## ]]\nwhy are my text messages coming up as maybe? [[ ## ground_truth ## ]]\nThis is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". However, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled. [[ ## system_response ## ]]\nYour text messages are showing up as \"maybe\" because your messaging app is uncertain about the sender's identity. This typically occurs when the sender's number is not saved in your contacts or if the message is from an unknown number. To resolve this, you can save the contact in your address book or check the message settings in your app. Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## ground_truth_key_ideas ## ]]`, then `[[ ## system_response_key_ideas ## ]]`, then `[[ ## discussion ## ]]`, then `[[ ## recall ## ]]` (must be formatted as a valid Python float), then `[[ ## precision ## ]]` (must be formatted as a valid Python float), and then ending with the marker for `[[ ## completed ## ]]`. Response: [[ ## reasoning ## ]]\nThe system response provides a general explanation for why text messages may show up as \"maybe,\" focusing on the uncertainty of the sender's identity. However, it does not mention the specific iOS features or the potential bug in iOS 11.2 that are highlighted in the ground truth. The ground truth explains that this feature is part of iOS 9's Proactivity features and mentions a specific bug in iOS 11.2, which is not addressed in the system response. [[ ## ground_truth_key_ideas ## ]]\n1. Proactivity features introduced in iOS 9.\n2. The system checks emails to associate phone numbers with contacts.\n3. \"Maybe\" is displayed if the number is not saved in contacts.\n4. Mention of a bug in iOS 11.2 causing \"Maybe\" to appear incorrectly. [[ ## system_response_key_ideas ## ]]\n1. Text messages show up as \"maybe\" due to uncertainty about the sender's identity.\n2. Occurs when the sender's number is not saved in contacts or is from an unknown number.\n3. Suggests saving the contact or checking message settings. [[ ## discussion ## ]]\nThere is some overlap between the ground truth and the system response regarding the uncertainty of the sender's identity and the suggestion to save the contact. However, the system response lacks specific details about the iOS features and the bug mentioned in the ground truth. The ground truth provides a more comprehensive explanation of the \"maybe\" feature, while the system response is more general and does not address the iOS version specifics. [[ ## recall ## ]]\n0.25 [[ ## precision ## ]]\n0.5 [[ ## completed ## ]] For evaluation, you could use the metric above in a simple loop and just average the score. But for nice parallelism and utilities, we can rely on dspy.Evaluate . In [11]: Copied! # Define an evaluator that we can re-use. evaluate = dspy . Evaluate ( devset = devset , metric = metric , num_threads = 24 , display_progress = True , display_table = 2 ) # Evaluate the Chain-of-Thought program. evaluate ( cot ) # Define an evaluator that we can re-use.\nevaluate = dspy.Evaluate(devset=devset, metric=metric, num_threads=24, display_progress=True, display_table=2) # Evaluate the Chain-of-Thought program.\nevaluate(cot) Average Metric: 125.68 / 300 (41.9%): 100%|██████████| 300/300 [00:00<00:00, 666.96it/s] 2024/11/23 23:16:36 INFO dspy.evaluate.evaluate: Average Metric: 125.68228336477591 / 300 (41.9%) question example_response gold_doc_ids reasoning pred_response SemanticF1 0 when to use c over c++, and c++ over c? If you are equally familiar with both C++ and C, it's advisable to... [733] C and C++ are both powerful programming languages, but they serve ... Use C when you need low-level access to memory, require high perfo... 1 should images be stored in a git repository? One viewpoint expresses that there is no significant downside, esp... [6253, 6254, 6275, 6278, 8215] Storing images in a Git repository can be beneficial for version c... Images can be stored in a Git repository, but it's important to co... ✔️ [0.444] ... 298 more rows not displayed ... Out[11]: 41.89 Tracking Evaluation Results in MLflow Experiment To track and visualize the evaluation results over time, you can record the results in MLflow Experiment. import mlflow with mlflow . start_run ( run_name = \"rag_evaluation\" ): evaluate = dspy . Evaluate ( devset = devset , metric = metric , num_threads = 24 , display_progress = True , ) # Evaluate the program as usual result = evaluate ( cot ) # Log the aggregated score mlflow . log_metric ( \"semantic_f1_score\" , result . score ) # Log the detailed evaluation results as a table mlflow . log_table ( { \"Question\" : [ example . question for example in eval_set ], \"Gold Response\" : [ example . response for example in eval_set ], \"Predicted Response\" : [ output [ 1 ] for output in result . results ], \"Semantic F1 Score\" : [ output [ 2 ] for output in result . results ], }, artifact_file = \"eval_results.json\" , ) To learn more about the integration, visit MLflow DSPy Documentation as well. So far, we built a very simple chain-of-thought module for question answering and evaluated it on a small dataset. Can we do better? In the rest of this guide, we will build a retrieval-augmented generation (RAG) program in DSPy for the same task. We'll see how this can boost the score substantially, then we'll use one of the DSPy Optimizers to compile our RAG program to higher-quality prompts, raising our scores even more. Basic Retrieval-Augmented Generation (RAG). ¶ First, let's download the corpus data that we will use for RAG search. An older version of this tutorial used the full (650,000 document) corpus. To make this very fast and cheap to run, we've downsampled the corpus to just 28,000 documents. In [12]: Copied! download ( \"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_corpus.jsonl\" ) download(\"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_corpus.jsonl\") Set up your system's retriever. ¶ As far as DSPy is concerned, you can plug in any Python code for calling tools or retrievers. Here, we'll just use OpenAI Embeddings and do top-K search locally, just for convenience. Note: The step below will require that you either do pip install -U faiss-cpu or pass brute_force_threshold=30_000 to dspy.retrievers.Embeddings to avoid faiss. In [13]: Copied! # %pip install -U faiss-cpu # or faiss-gpu if you have a GPU # %pip install -U faiss-cpu # or faiss-gpu if you have a GPU In [14]: Copied! max_characters = 6000 # for truncating >99th percentile of documents topk_docs_to_retrieve = 5 # number of documents to retrieve per search query with open ( \"ragqa_arena_tech_corpus.jsonl\" ) as f : corpus = [ orjson . loads ( line )[ 'text' ][: max_characters ] for line in f ] print ( f \"Loaded { len ( corpus ) } documents. Will encode them below.\" ) embedder = dspy . Embedder ( 'openai/text-embedding-3-small' , dimensions = 512 ) search = dspy . retrievers . Embeddings ( embedder = embedder , corpus = corpus , k = topk_docs_to_retrieve ) max_characters = 6000 # for truncating >99th percentile of documents\ntopk_docs_to_retrieve = 5 # number of documents to retrieve per search query with open(\"ragqa_arena_tech_corpus.jsonl\") as f: corpus = [orjson.loads(line)['text'][:max_characters] for line in f] print(f\"Loaded {len(corpus)} documents. Will encode them below.\") embedder = dspy.Embedder('openai/text-embedding-3-small', dimensions=512)\nsearch = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve) Loaded 28436 documents. Will encode them below.\nTraining a 32-byte FAISS index with 337 partitions, based on 28436 x 512-dim embeddings Build your first RAG Module. ¶ In the previous guide, we looked at individual DSPy modules in isolation, e.g. dspy.Predict(\"question -> answer\") . What if we want to build a DSPy program that has multiple steps? The syntax below with dspy.Module allows you to connect a few pieces together, in this case, our retriever and a generation module, so the whole system can be optimized. Concretely, in the __init__ method, you declare any sub-module you'll need, which in this case is just a dspy.ChainOfThought('context, question -> response') module that takes retrieved context, a question, and produces a response. In the forward method, you simply express any Python control flow you like, possibly using your modules. In this case, we first invoke the search function defined earlier and then invoke the self.respond ChainOfThought module. In [15]: Copied! class RAG ( dspy . Module ): def __init__ ( self ): self . respond = dspy . ChainOfThought ( 'context, question -> response' ) def forward ( self , question ): context = search ( question ) . passages return self . respond ( context = context , question = question ) class RAG(dspy.Module): def __init__(self): self.respond = dspy.ChainOfThought('context, question -> response') def forward(self, question): context = search(question).passages return self.respond(context=context, question=question) Let's use the RAG module. In [16]: Copied! rag = RAG () rag ( question = \"what are high memory and low memory on linux?\" ) rag = RAG()\nrag(question=\"what are high memory and low memory on linux?\") Out[16]: Prediction( reasoning=\"High Memory and Low Memory in Linux refer to two segments of the kernel's memory space. Low Memory is the portion of memory that the kernel can access directly and is statically mapped at boot time. This area is typically used for kernel data structures and is always accessible to the kernel. High Memory, on the other hand, is not permanently mapped in the kernel's address space, meaning that the kernel cannot access it directly without first mapping it into its address space. High Memory is used for user-space applications and temporary data buffers. The distinction allows for better memory management and security, as user-space applications cannot directly access kernel-space memory.\", response=\"In Linux, High Memory refers to the segment of memory that is not permanently mapped in the kernel's address space, which means the kernel must map it temporarily to access it. This area is typically used for user-space applications and temporary data buffers. Low Memory, in contrast, is the portion of memory that the kernel can access directly and is statically mapped at boot time. It is used for kernel data structures and is always accessible to the kernel. This separation enhances security by preventing user-space applications from accessing kernel-space memory directly.\"\n) In [17]: Copied! dspy . inspect_history () dspy.inspect_history() [2024-11-23T23:16:49.175612] System message: Your input fields are:\n1. `context` (str)\n2. `question` (str) Your output fields are:\n1. `reasoning` (str)\n2. `response` (str) All interactions will be structured in the following way, with the appropriate values filled in. [[ ## context ## ]]\n{context} [[ ## question ## ]]\n{question} [[ ## reasoning ## ]]\n{reasoning} [[ ## response ## ]]\n{response} [[ ## completed ## ]] In adhering to this structure, your objective is: Given the fields `context`, `question`, produce the fields `response`. User message: [[ ## context ## ]]\n[1] «As far as I remember, High Memory is used for application space and Low Memory for the kernel. Advantage is that (user-space) applications cant access kernel-space memory.»\n[2] «HIGHMEM is a range of kernels memory space, but it is NOT memory you access but its a place where you put what you want to access. A typical 32bit Linux virtual memory map is like: 0x00000000-0xbfffffff: user process (3GB) 0xc0000000-0xffffffff: kernel space (1GB) (CPU-specific vector and whatsoever are ignored here). Linux splits the 1GB kernel space into 2 pieces, LOWMEM and HIGHMEM. The split varies from installation to installation. If an installation chooses, say, 512MB-512MB for LOW and HIGH mems, the 512MB LOWMEM (0xc0000000-0xdfffffff) is statically mapped at the kernel boot time; usually the first so many bytes of the physical memory is used for this so that virtual and physical addresses in this range have a constant offset of, say, 0xc0000000. On the other hand, the latter 512MB (HIGHMEM) has no static mapping (although you could leave pages semi-permanently mapped there, but you must do so explicitly in your driver code). Instead, pages are temporarily mapped and unmapped here so that virtual and physical addresses in this range have no consistent mapping. Typical uses of HIGHMEM include single-time data buffers.»\n[3] «This is relevant to the Linux kernel; Im not sure how any Unix kernel handles this. The High Memory is the segment of memory that user-space programs can address. It cannot touch Low Memory. Low Memory is the segment of memory that the Linux kernel can address directly. If the kernel must access High Memory, it has to map it into its own address space first. There was a patch introduced recently that lets you control where the segment is. The tradeoff is that you can take addressable memory away from user space so that the kernel can have more memory that it does not have to map before using. Additional resources: http://tldp.org/HOWTO/KernelAnalysis-HOWTO-7.html http://linux-mm.org/HighMemory»\n[4] «The first reference to turn to is Linux Device Drivers (available both online and in book form), particularly chapter 15 which has a section on the topic. In an ideal world, every system component would be able to map all the memory it ever needs to access. And this is the case for processes on Linux and most operating systems: a 32-bit process can only access a little less than 2^32 bytes of virtual memory (in fact about 3GB on a typical Linux 32-bit architecture). It gets difficult for the kernel, which needs to be able to map the full memory of the process whose system call its executing, plus the whole physical memory, plus any other memory-mapped hardware device. So when a 32-bit kernel needs to map more than 4GB of memory, it must be compiled with high memory support. High memory is memory which is not permanently mapped in the kernels address space. (Low memory is the opposite: it is always mapped, so you can access it in the kernel simply by dereferencing a pointer.) When you access high memory from kernel code, you need to call kmap first, to obtain a pointer from a page data structure (struct page). Calling kmap works whether the page is in high or low memory. There is also kmap_atomic which has added constraints but is more efficient on multiprocessor machines because it uses finer-grained locking. The pointer obtained through kmap is a resource: it uses up address space. Once youve finished with it, you must call kunmap (or kunmap_atomic) to free that resource; then the pointer is no longer valid, and the contents of the page cant be accessed until you call kmap again.»\n[5] «/proc/meminfo will tell you how free works, but /proc/kcore can tell you what the kernel uses. From the same page: /proc/kcore This file represents the physical memory of the system and is stored in the ELF core file format. With this pseudo-file, and an unstripped kernel (/usr/src/linux/vmlinux) binary, GDB can be used to examine the current state of any kernel data structures. The total length of the file is the size of physical memory (RAM) plus 4KB. /proc/meminfo This file reports statistics about memory usage on the system. It is used by free(1) to report the amount of free and used memory (both physical and swap) on the system as well as the shared memory and buffers used by the kernel. Each line of the file consists of a parameter name, followed by a colon, the value of the parameter, and an option unit of measurement (e.g., kB). The list below describes the parameter names and the format specifier required to read the field value. Except as noted below, all of the fields have been present since at least Linux 2.6.0. Some fields are displayed only if the kernel was configured with various options; those dependencies are noted in the list. MemTotal %lu Total usable RAM (i.e., physical RAM minus a few reserved bits and the kernel binary code). MemFree %lu The sum of LowFree+HighFree. Buffers %lu Relatively temporary storage for raw disk blocks that shouldnt get tremendously large (20MB or so). Cached %lu In-memory cache for files read from the disk (the page cache). Doesnt include SwapCached. SwapCached %lu Memory that once was swapped out, is swapped back in but still also is in the swap file. (If memory pressure is high, these pages dont need to be swapped out again because they are already in the swap file. This saves I/O.) Active %lu Memory that has been used more recently and usually not reclaimed unless absolutely necessary. Inactive %lu Memory which has been less recently used. It is more eligible to be reclaimed for other purposes. Active(anon) %lu (since Linux 2.6.28) [To be documented.] Inactive(anon) %lu (since Linux 2.6.28) [To be documented.] Active(file) %lu (since Linux 2.6.28) [To be documented.] Inactive(file) %lu (since Linux 2.6.28) [To be documented.] Unevictable %lu (since Linux 2.6.28) (From Linux 2.6.28 to 2.6.30, CONFIG_UNEVICTABLE_LRU was required.) [To be documented.] Mlocked %lu (since Linux 2.6.28) (From Linux 2.6.28 to 2.6.30, CONFIG_UNEVICTABLE_LRU was required.) [To be documented.] HighTotal %lu (Starting with Linux 2.6.19, CONFIG_HIGHMEM is required.) Total amount of highmem. Highmem is all memory above ~860MB of physical memory. Highmem areas are for use by user-space programs, or for the page cache. The kernel must use tricks to access this memory, making it slower to access than lowmem. HighFree %lu (Starting with Linux 2.6.19, CONFIG_HIGHMEM is required.) Amount of free highmem. LowTotal %lu (Starting with Linux 2.6.19, CONFIG_HIGHMEM is required.) Total amount of lowmem. Lowmem is memory which can be used for everything that highmem can be used for, but it is also available for the kernels use for its own data structures. Among many other things, it is where everything from Slab is allocated. Bad things happen when you're out of lowmem. LowFree %lu (Starting with Linux 2.6.19, CONFIG_HIGHMEM is required.) Amount of free lowmem. MmapCopy %lu (since Linux 2.6.29) (CONFIG_MMU is required.) [To be documented.] SwapTotal %lu Total amount of swap space available. SwapFree %lu Amount of swap space that is currently unused. Dirty %lu Memory which is waiting to get written back to the disk. Writeback %lu Memory which is actively being written back to the disk. AnonPages %lu (since Linux 2.6.18) Non-file backed pages mapped into user-space page tables. Mapped %lu Files which have been mapped, such as libraries. Shmem %lu (since Linux 2.6.32) [To be documented.] Slab %lu In-kernel data structures cache. SReclaimable %lu (since Linux 2.6.19) Part of Slab, that might be reclaimed, such as caches. SUnreclaim %lu (since Linux 2.6.19) Part of Slab, that cannot be reclaimed on memory pressure. KernelStack %lu (since Linux 2.6.32) Amount of memory allocated to kernel stacks. PageTables %lu (since Linux 2.6.18) Amount of memory dedicated to the lowest level of page tables. Quicklists %lu (since Linux 2.6.27) (CONFIG_QUICKLIST is required.) [To be documented.] NFS_Unstable %lu (since Linux 2.6.18) NFS pages sent to the server, but not yet committed to stable storage. Bounce %lu (since Linux 2.6.18) Memory used for block device bounce buffers. WritebackTmp %lu (since Linux 2.6.26) Memory used by FUSE for temporary writeback buffers. CommitLimit %lu (since Linux 2.6.10) Based on the overcommit ratio (vm.overcommit_ratio), this is the total amount of memory currently available to be allocated on the system. This limit is adhered to only if strict overcommit accounting is enabled (mode 2 in /proc/sys/vm/overcommit_ratio). The CommitLimit is calculated using the following formula: CommitLimit = ([total RAM pages] - [total huge TLB pages]) * overcommit_ratio / 100 + [total swap pages] For example, on a system with 1GB of physical RAM and 7GB of swap with a overcommit_ratio of 30, this formula yields a CommitLimit of 7.3GB. For more details, see the memory overcommit documentation in the kernel source file Documentation/vm/overcommit-accounting. Committed_AS %lu The amount of memory presently allocated on the system. The committed memory is a sum of all of the memory which has been allocated by processes, even if it has not been used by them as of yet. A process which allocates 1GB of memory (using malloc(3) or similar), but touches only 300MB of that memory will show up as using only 300MB of memory even if it has the address space allocated for the entire 1GB. This 1GB is memory which has been committed to by the VM and can be used at any time by the allocating application. With strict overcommit enabled on the system (mode 2 /proc/sys/vm/overcommit_memory), allocations w» [[ ## question ## ]]\nwhat are high memory and low memory on linux? Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`. Response: [[ ## reasoning ## ]]\nHigh Memory and Low Memory in Linux refer to two segments of the kernel's memory space. Low Memory is the portion of memory that the kernel can access directly and is statically mapped at boot time. This area is typically used for kernel data structures and is always accessible to the kernel. High Memory, on the other hand, is not permanently mapped in the kernel's address space, meaning that the kernel cannot access it directly without first mapping it into its address space. High Memory is used for user-space applications and temporary data buffers. The distinction allows for better memory management and security, as user-space applications cannot directly access kernel-space memory. [[ ## response ## ]]\nIn Linux, High Memory refers to the segment of memory that is not permanently mapped in the kernel's address space, which means the kernel must map it temporarily to access it. This area is typically used for user-space applications and temporary data buffers. Low Memory, in contrast, is the portion of memory that the kernel can access directly and is statically mapped at boot time. It is used for kernel data structures and is always accessible to the kernel. This separation enhances security by preventing user-space applications from accessing kernel-space memory directly. [[ ## completed ## ]] Earlier with a CoT module, we got around 40% in terms of semantic F1 on our devset . Would this RAG module score better? In [18]: Copied! evaluate ( RAG ()) evaluate(RAG()) Average Metric: 166.54 / 300 (55.5%): 100%|██████████| 300/300 [00:04<00:00, 61.40it/s] 2024/11/23 23:16:54 INFO dspy.evaluate.evaluate: Average Metric: 166.53601368289284 / 300 (55.5%) question example_response gold_doc_ids reasoning pred_response SemanticF1 0 when to use c over c++, and c++ over c? If you are equally familiar with both C++ and C, it's advisable to... [733] C should be used over C++ primarily in scenarios where simplicity ... Use C over C++ when working on embedded systems, requiring low-lev... ✔️ [0.500] 1 should images be stored in a git repository? One viewpoint expresses that there is no significant downside, esp... [6253, 6254, 6275, 6278, 8215] Storing images in a Git repository is generally not recommended du... While it is technically possible to store images in a Git reposito... ✔️ [0.444] ... 298 more rows not displayed ... Out[18]: 55.51 Using a DSPy Optimizer to improve your RAG prompt. ¶ Off the shelf, our RAG module scores 55%. What are our options to make it stronger? One of the various choices DSPy offers is optimizing the prompts in our pipeline. If there are many sub-modules in your program, all of them will be optimized together. In this case, there's only one: self.respond = dspy.ChainOfThought('context, question -> response') Let's set up and use DSPy's MIPRO (v2) optimizer. The run below has a cost around $1.5 (for the medium auto setting) and may take some 20-30 minutes depending on your number of threads. In [ ]: Copied! tp = dspy . MIPROv2 ( metric = metric , auto = \"medium\" , num_threads = 24 ) # use fewer threads if your rate limit is small optimized_rag = tp . compile ( RAG (), trainset = trainset , max_bootstrapped_demos = 2 , max_labeled_demos = 2 ) tp = dspy.MIPROv2(metric=metric, auto=\"medium\", num_threads=24) # use fewer threads if your rate limit is small optimized_rag = tp.compile(RAG(), trainset=trainset, max_bootstrapped_demos=2, max_labeled_demos=2) The prompt optimization process here is pretty systematic, you can learn about it for example in this paper . Importantly, it's not a magic button. It's very possible that it can overfit your training set for instance and not generalize well to a held-out set, making it essential that we iteratively validate our programs. Let's check on an example here, asking the same question to the baseline rag = RAG() program, which was not optimized, and to the optimized_rag = MIPROv2(..)(..) program, after prompt optimization. In [20]: Copied! baseline = rag ( question = \"cmd+tab does not work on hidden or minimized windows\" ) print ( baseline . response ) baseline = rag(question=\"cmd+tab does not work on hidden or minimized windows\")\nprint(baseline.response) You are correct that cmd+tab does not work on hidden or minimized windows. To switch back to a minimized app, you must first switch to another application and let it take focus before returning to the minimized one. In [21]: Copied! pred = optimized_rag ( question = \"cmd+tab does not work on hidden or minimized windows\" ) print ( pred . response ) pred = optimized_rag(question=\"cmd+tab does not work on hidden or minimized windows\")\nprint(pred.response) The Command + Tab shortcut on macOS is designed to switch between currently open applications, but it does not directly restore minimized or hidden windows. When you use Command + Tab, it cycles through the applications that are actively running, and minimized windows do not count as active. To manage minimized windows, you can use other shortcuts or methods. For example, you can use Command + Option + H + M to hide all other applications and minimize the most recently used one. Alternatively, you can navigate to the application you want to restore using Command + Tab and then manually click on the minimized window in the Dock to bring it back to focus. You can use dspy.inspect_history(n=2) to view the RAG prompt before optimization and after optimization . Concretely, in one of the runs of this notebook, the optimized prompt does the following (note that it may be different on a later rerun). Constructs the following instruction, Using the provided `context` and `question`, analyze the information step by step to generate a comprehensive and informative `response`. Ensure that the response clearly explains the concepts involved, highlights key distinctions, and addresses any complexities noted in the context. And includes two fully worked out RAG examples with synthetic reasoning and answers, e.g. how to transfer whatsapp voice message to computer? . Let's now evaluate on the overall devset. In [22]: Copied! evaluate ( optimized_rag ) evaluate(optimized_rag) Average Metric: 183.32 / 300 (61.1%): 100%|██████████| 300/300 [00:02<00:00, 104.48it/s] 2024/11/23 23:17:21 INFO dspy.evaluate.evaluate: Average Metric: 183.3194433591069 / 300 (61.1%) question example_response gold_doc_ids reasoning pred_response SemanticF1 0 when to use c over c++, and c++ over c? If you are equally familiar with both C++ and C, it's advisable to... [733] The context provides insights into the strengths and weaknesses of... You should consider using C over C++ in scenarios where simplicity... ✔️ [0.333] 1 should images be stored in a git repository? One viewpoint expresses that there is no significant downside, esp... [6253, 6254, 6275, 6278, 8215] The context discusses the challenges and considerations of storing... Storing images in a Git repository is generally considered bad pra... ✔️ [0.500] ... 298 more rows not displayed ... Out[22]: 61.11 Keeping an eye on cost. ¶ DSPy allows you to track the cost of your programs, which can be used to monitor the cost of your calls. Here, we'll show you how to track the cost of your programs with DSPy. In [23]: Copied! cost = sum ([ x [ 'cost' ] for x in lm . history if x [ 'cost' ] is not None ]) # in USD, as calculated by LiteLLM for certain providers cost = sum([x['cost'] for x in lm.history if x['cost'] is not None]) # in USD, as calculated by LiteLLM for certain providers Saving and loading. ¶ The optimized program has a pretty simple structure on the inside. Feel free to explore it. Here, we'll save optimized_rag so we can load it again later without having to optimize from scratch. In [24]: Copied! optimized_rag . save ( \"optimized_rag.json\" ) loaded_rag = RAG () loaded_rag . load ( \"optimized_rag.json\" ) loaded_rag ( question = \"cmd+tab does not work on hidden or minimized windows\" ) optimized_rag.save(\"optimized_rag.json\") loaded_rag = RAG()\nloaded_rag.load(\"optimized_rag.json\") loaded_rag(question=\"cmd+tab does not work on hidden or minimized windows\") Out[24]: Prediction( reasoning='The context explains how the Command + Tab shortcut functions on macOS, particularly in relation to switching between applications. It notes that this shortcut does not bring back minimized or hidden windows directly. Instead, it cycles through applications that are currently open and visible. The information also suggests alternative methods for managing minimized windows and provides insights into how to navigate between applications effectively.', response='The Command + Tab shortcut on macOS is designed to switch between currently open applications, but it does not directly restore minimized or hidden windows. When you use Command + Tab, it cycles through the applications that are actively running, and minimized windows do not count as active. To manage minimized windows, you can use other shortcuts or methods. For example, you can use Command + Option + H + M to hide all other applications and minimize the most recently used one. Alternatively, you can navigate to the application you want to restore using Command + Tab and then manually click on the minimized window in the Dock to bring it back to focus.'\n) Saving programs in MLflow Experiment Instead of saving the program to a local file, you can track it in MLflow for better reproducibility and collaboration. Dependency Management : MLflow automatically save the frozen environment metadata along with the program to ensure reproducibility. Experiment Tracking : With MLflow, you can track the program's performance and cost along with the program itself. Collaboration : You can share the program and results with your team members by sharing the MLflow experiment. To save the program in MLflow, run the following code: import mlflow # Start an MLflow Run and save the program with mlflow . start_run ( run_name = \"optimized_rag\" ): model_info = mlflow . dspy . log_model ( optimized_rag , artifact_path = \"model\" , # Any name to save the program in MLflow ) # Load the program back from MLflow loaded = mlflow . dspy . load_model ( model_info . model_uri ) To learn more about the integration, visit MLflow DSPy Documentation as well. What's next? ¶ Improving from around 42% to approximately 61% on this task, in terms of SemanticF1 , was pretty easy. But DSPy gives you paths to continue iterating on the quality of your system and we have barely scratched the surface. In general, you have the following tools: Explore better system architectures for your program, e.g. what if we ask the LM to generate search queries for the retriever? See, e.g., the STORM pipeline built in DSPy. Explore different prompt optimizers or weight optimizers . See the Optimizers Docs. Scale inference time compute using DSPy Optimizers, e.g. via ensembling multiple post-optimization programs. Cut cost by distilling to a smaller LM, via prompt or weight optimization. How do you decide which ones to proceed with first? The first step is to look at your system outputs, which will allow you to identify the sources of lower performance if any. While doing all of this, make sure you continue to refine your metric, e.g. by optimizing against your judgments, and to collect more (or more realistic) data, e.g. from related domains or from putting a demo of your system in front of users. Back to top", "meta": {"tags": ["url"], "source_id": "dspy_rag_tutorial", "source_type": "url", "url": "https://dspy.ai/tutorials/rag/", "format": null, "loader_options_used": {}}}
{"id": "example_blog_post", "title": "Beautiful Soup Documentation", "source_type": "url", "source": "https://beautiful-soup-4.readthedocs.io/en/latest/", "text": "Docs » Beautiful Soup Documentation View page source Beautiful Soup Documentation Â¶ Beautiful Soup is a\nPython library for pulling data out of HTML and XML files. It works\nwith your favorite parser to provide idiomatic ways of navigating,\nsearching, and modifying the parse tree. It commonly saves programmers\nhours or days of work. These instructions illustrate all major features of Beautiful Soup 4,\nwith examples. I show you what the library is good for, how it works,\nhow to use it, how to make it do what you want, and what to do when it\nviolates your expectations. This document covers Beautiful Soup version 4.8.1. The examples in\nthis documentation should work the same way in Python 2.7 and Python\n3.2. You might be looking for the documentation for Beautiful Soup 3 .\nIf so, you should know that Beautiful Soup 3 is no longer being\ndeveloped and that support for it will be dropped on or after December\n31, 2020. If you want to learn about the differences between Beautiful\nSoup 3 and Beautiful Soup 4, see Porting code to BS4 . This documentation has been translated into other languages by\nBeautiful Soup users: è¿ç¯ææ¡£å½ç¶è¿æä¸­æç. ãã®ãã¼ã¸ã¯æ¥æ¬èªã§å©ç¨ã§ãã¾ã( å¤é¨ãªã³ã¯ ) ì´ ë¬¸ìë íêµ­ì´ ë²ì­ë ê°ë¥í©ëë¤. Este documento tambÃ©m estÃ¡ disponÃ­vel em PortuguÃªs do Brasil. Getting help Â¶ If you have questions about Beautiful Soup, or run into problems, send mail to the discussion group . If\nyour problem involves parsing an HTML document, be sure to mention what the diagnose() function says about\nthat document. Quick Start Â¶ Hereâs an HTML document Iâll be using as an example throughout this\ndocument. Itâs part of a story from Alice in Wonderland : html_doc = \"\"\" <html><head><title>The Dormouse's story</title></head> <body> <p class=\"title\"><b>The Dormouse's story</b></p> <p class=\"story\">Once upon a time there were three little sisters; and their names were <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>, <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and <a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>; and they lived at the bottom of a well.</p> <p class=\"story\">...</p> \"\"\" Running the âthree sistersâ document through Beautiful Soup gives us a BeautifulSoup object, which represents the document as a nested\ndata structure: from bs4 import BeautifulSoup soup = BeautifulSoup ( html_doc , 'html.parser' ) print ( soup . prettify ()) # <html> # <head> # <title> # The Dormouse's story # </title> # </head> # <body> # <p class=\"title\"> # <b> # The Dormouse's story # </b> # </p> # <p class=\"story\"> # Once upon a time there were three little sisters; and their names were # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> # Elsie # </a> # , # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> # Lacie # </a> # and # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link2\"> # Tillie # </a> # ; and they lived at the bottom of a well. # </p> # <p class=\"story\"> # ... # </p> # </body> # </html> Here are some simple ways to navigate that data structure: soup . title # <title>The Dormouse's story</title> soup . title . name # u'title' soup . title . string # u'The Dormouse's story' soup . title . parent . name # u'head' soup . p # <p class=\"title\"><b>The Dormouse's story</b></p> soup . p [ 'class' ] # u'title' soup . a # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> soup . find_all ( 'a' ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] soup . find ( id = \"link3\" ) # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a> One common task is extracting all the URLs found within a pageâs <a> tags: for link in soup . find_all ( 'a' ): print ( link . get ( 'href' )) # http://example.com/elsie # http://example.com/lacie # http://example.com/tillie Another common task is extracting all the text from a page: print ( soup . get_text ()) # The Dormouse's story # # The Dormouse's story # # Once upon a time there were three little sisters; and their names were # Elsie, # Lacie and # Tillie; # and they lived at the bottom of a well. # # ... Does this look like what you need? If so, read on. Installing Beautiful Soup Â¶ If youâre using a recent version of Debian or Ubuntu Linux, you can\ninstall Beautiful Soup with the system package manager: $ apt-get install python-bs4 (for Python 2) $ apt-get install python3-bs4 (for Python 3) Beautiful Soup 4 is published through PyPi, so if you canât install it\nwith the system packager, you can install it with easy_install or pip . The package name is beautifulsoup4 , and the same package\nworks on Python 2 and Python 3. Make sure you use the right version of pip or easy_install for your Python version (these may be named pip3 and easy_install3 respectively if youâre using Python 3). $ easy_install beautifulsoup4 $ pip install beautifulsoup4 (The BeautifulSoup package is probably not what you want. Thatâs\nthe previous major release, Beautiful Soup 3 . Lots of software uses\nBS3, so itâs still available, but if youâre writing new code you\nshould install beautifulsoup4 .) If you donât have easy_install or pip installed, you can download the Beautiful Soup 4 source tarball and\ninstall it with setup.py . $ python setup.py install If all else fails, the license for Beautiful Soup allows you to\npackage the entire library with your application. You can download the\ntarball, copy its bs4 directory into your applicationâs codebase,\nand use Beautiful Soup without installing it at all. I use Python 2.7 and Python 3.2 to develop Beautiful Soup, but it\nshould work with other recent versions. Problems after installation Â¶ Beautiful Soup is packaged as Python 2 code. When you install it for\nuse with Python 3, itâs automatically converted to Python 3 code. If\nyou donât install the package, the code wonât be converted. There have\nalso been reports on Windows machines of the wrong version being\ninstalled. If you get the ImportError âNo module named HTMLParserâ, your\nproblem is that youâre running the Python 2 version of the code under\nPython 3. If you get the ImportError âNo module named html.parserâ, your\nproblem is that youâre running the Python 3 version of the code under\nPython 2. In both cases, your best bet is to completely remove the Beautiful\nSoup installation from your system (including any directory created\nwhen you unzipped the tarball) and try the installation again. If you get the SyntaxError âInvalid syntaxâ on the line ROOT_TAG_NAME = u'[document]' , you need to convert the Python 2\ncode to Python 3. You can do this either by installing the package: $ python3 setup.py install or by manually running Pythonâs 2to3 conversion script on the bs4 directory: $ 2to3-3.2 -w bs4 Installing a parser Â¶ Beautiful Soup supports the HTML parser included in Pythonâs standard\nlibrary, but it also supports a number of third-party Python parsers.\nOne is the lxml parser . Depending on your setup,\nyou might install lxml with one of these commands: $ apt-get install python-lxml $ easy_install lxml $ pip install lxml Another alternative is the pure-Python html5lib parser , which parses HTML the way a\nweb browser does. Depending on your setup, you might install html5lib\nwith one of these commands: $ apt-get install python-html5lib $ easy_install html5lib $ pip install html5lib This table summarizes the advantages and disadvantages of each parser library: Parser Typical usage Advantages Disadvantages Pythonâs html.parser BeautifulSoup(markup, \"html.parser\") Batteries included Decent speed Lenient (As of Python 2.7.3\nand 3.2.) Not as fast as lxml,\nless lenient than\nhtml5lib. lxmlâs HTML parser BeautifulSoup(markup, \"lxml\") Very fast Lenient External C dependency lxmlâs XML parser BeautifulSoup(markup, \"lxml-xml\") BeautifulSoup(markup, \"xml\") Very fast The only currently supported\nXML parser External C dependency html5lib BeautifulSoup(markup, \"html5lib\") Extremely lenient Parses pages the same way a\nweb browser does Creates valid HTML5 Very slow External Python\ndependency If you can, I recommend you install and use lxml for speed. If youâre\nusing a version of Python 2 earlier than 2.7.3, or a version of Python\n3 earlier than 3.2.2, itâs essential that you install lxml or\nhtml5libâPythonâs built-in HTML parser is just not very good in older\nversions. Note that if a document is invalid, different parsers will generate\ndifferent Beautiful Soup trees for it. See Differences\nbetween parsers for details. Making the soup Â¶ To parse a document, pass it into the BeautifulSoup constructor. You can pass in a string or an open filehandle: from bs4 import BeautifulSoup with open ( \"index.html\" ) as fp : soup = BeautifulSoup ( fp ) soup = BeautifulSoup ( \"<html>data</html>\" ) First, the document is converted to Unicode, and HTML entities are\nconverted to Unicode characters: BeautifulSoup(\"Sacr&eacute; bleu!\")\n<html><head></head><body>SacrÃ© bleu!</body></html> Beautiful Soup then parses the document using the best available\nparser. It will use an HTML parser unless you specifically tell it to\nuse an XML parser. (See Parsing XML .) Kinds of objects Â¶ Beautiful Soup transforms a complex HTML document into a complex tree\nof Python objects. But youâll only ever have to deal with about four kinds of objects: Tag , NavigableString , BeautifulSoup ,\nand Comment . Tag Â¶ A Tag object corresponds to an XML or HTML tag in the original document: soup = BeautifulSoup ( '<b class=\"boldest\">Extremely bold</b>' ) tag = soup . b type ( tag ) # <class 'bs4.element.Tag'> Tags have a lot of attributes and methods, and Iâll cover most of them\nin Navigating the tree and Searching the tree . For now, the most\nimportant features of a tag are its name and attributes. Name Â¶ Every tag has a name, accessible as .name : tag . name # u'b' If you change a tagâs name, the change will be reflected in any HTML\nmarkup generated by Beautiful Soup: tag . name = \"blockquote\" tag # <blockquote class=\"boldest\">Extremely bold</blockquote> Attributes Â¶ A tag may have any number of attributes. The tag <b id=\"boldest\"> has an attribute âidâ whose value is\nâboldestâ. You can access a tagâs attributes by treating the tag like\na dictionary: tag [ 'id' ] # u'boldest' You can access that dictionary directly as .attrs : tag . attrs # {u'id': 'boldest'} You can add, remove, and modify a tagâs attributes. Again, this is\ndone by treating the tag as a dictionary: tag [ 'id' ] = 'verybold' tag [ 'another-attribute' ] = 1 tag # <b another-attribute=\"1\" id=\"verybold\"></b> del tag [ 'id' ] del tag [ 'another-attribute' ] tag # <b></b> tag [ 'id' ] # KeyError: 'id' print ( tag . get ( 'id' )) # None Multi-valued attributes Â¶ HTML 4 defines a few attributes that can have multiple values. HTML 5\nremoves a couple of them, but defines a few more. The most common\nmulti-valued attribute is class (that is, a tag can have more than\none CSS class). Others include rel , rev , accept-charset , headers , and accesskey . Beautiful Soup presents the value(s)\nof a multi-valued attribute as a list: css_soup = BeautifulSoup ( '<p class=\"body\"></p>' ) css_soup . p [ 'class' ] # [\"body\"] css_soup = BeautifulSoup ( '<p class=\"body strikeout\"></p>' ) css_soup . p [ 'class' ] # [\"body\", \"strikeout\"] If an attribute looks like it has more than one value, but itâs not\na multi-valued attribute as defined by any version of the HTML\nstandard, Beautiful Soup will leave the attribute alone: id_soup = BeautifulSoup ( '<p id=\"my id\"></p>' ) id_soup . p [ 'id' ] # 'my id' When you turn a tag back into a string, multiple attribute values are\nconsolidated: rel_soup = BeautifulSoup ( '<p>Back to the <a rel=\"index\">homepage</a></p>' ) rel_soup . a [ 'rel' ] # ['index'] rel_soup . a [ 'rel' ] = [ 'index' , 'contents' ] print ( rel_soup . p ) # <p>Back to the <a rel=\"index contents\">homepage</a></p> You can disable this by passing multi_valued_attributes=None as a\nkeyword argument into the BeautifulSoup constructor: no_list_soup = BeautifulSoup ( '<p class=\"body strikeout\"></p>' , 'html' , multi_valued_attributes = None ) no_list_soup . p [ 'class' ] # u'body strikeout' You can use `get_attribute_list to get a value thatâs always a\nlist, whether or not itâs a multi-valued atribute: id_soup . p . get_attribute_list ( 'id' ) # [\"my id\"] If you parse a document as XML, there are no multi-valued attributes: xml_soup = BeautifulSoup ( '<p class=\"body strikeout\"></p>' , 'xml' ) xml_soup . p [ 'class' ] # u'body strikeout' Again, you can configure this using the multi_valued_attributes argument: class_is_multi = { '*' : 'class' } xml_soup = BeautifulSoup ( '<p class=\"body strikeout\"></p>' , 'xml' , multi_valued_attributes = class_is_multi ) xml_soup . p [ 'class' ] # [u'body', u'strikeout'] You probably wonât need to do this, but if you do, use the defaults as\na guide. They implement the rules described in the HTML specification: from bs4.builder import builder_registry builder_registry . lookup ( 'html' ) . DEFAULT_CDATA_LIST_ATTRIBUTES NavigableString Â¶ A string corresponds to a bit of text within a tag. Beautiful Soup\nuses the NavigableString class to contain these bits of text: tag . string # u'Extremely bold' type ( tag . string ) # <class 'bs4.element.NavigableString'> A NavigableString is just like a Python Unicode string, except\nthat it also supports some of the features described in Navigating\nthe tree and Searching the tree . You can convert a NavigableString to a Unicode string with unicode() : unicode_string = unicode ( tag . string ) unicode_string # u'Extremely bold' type ( unicode_string ) # <type 'unicode'> You canât edit a string in place, but you can replace one string with\nanother, using replace_with() : tag . string . replace_with ( \"No longer bold\" ) tag # <blockquote>No longer bold</blockquote> NavigableString supports most of the features described in Navigating the tree and Searching the tree , but not all of\nthem. In particular, since a string canât contain anything (the way a\ntag may contain a string or another tag), strings donât support the .contents or .string attributes, or the find() method. If you want to use a NavigableString outside of Beautiful Soup,\nyou should call unicode() on it to turn it into a normal Python\nUnicode string. If you donât, your string will carry around a\nreference to the entire Beautiful Soup parse tree, even when youâre\ndone using Beautiful Soup. This is a big waste of memory. BeautifulSoup Â¶ The BeautifulSoup object represents the parsed document as a\nwhole. For most purposes, you can treat it as a Tag object. This means it supports most of the methods described in Navigating the tree and Searching the tree . You can also pass a BeautifulSoup object into one of the methods\ndefined in Modifying the tree , just as you would a Tag . This\nlets you do things like combine two parsed documents: doc = BeautifulSoup ( \"<document><content/>INSERT FOOTER HERE</document\" , \"xml\" ) footer = BeautifulSoup ( \"<footer>Here's the footer</footer>\" , \"xml\" ) doc . find ( text = \"INSERT FOOTER HERE\" ) . replace_with ( footer ) # u'INSERT FOOTER HERE' print ( doc ) # <?xml version=\"1.0\" encoding=\"utf-8\"?> # <document><content/><footer>Here's the footer</footer></document> Since the BeautifulSoup object doesnât correspond to an actual\nHTML or XML tag, it has no name and no attributes. But sometimes itâs\nuseful to look at its .name , so itâs been given the special .name â[document]â: soup . name # u'[document]' Comments and other special strings Â¶ Tag , NavigableString , and BeautifulSoup cover almost\neverything youâll see in an HTML or XML file, but there are a few\nleftover bits. The only one youâll probably ever need to worry about\nis the comment: markup = \"<b><!--Hey, buddy. Want to buy a used parser?--></b>\" soup = BeautifulSoup ( markup ) comment = soup . b . string type ( comment ) # <class 'bs4.element.Comment'> The Comment object is just a special type of NavigableString : comment # u'Hey, buddy. Want to buy a used parser' But when it appears as part of an HTML document, a Comment is\ndisplayed with special formatting: print ( soup . b . prettify ()) # <b> # <!--Hey, buddy. Want to buy a used parser?--> # </b> Beautiful Soup defines classes for anything else that might show up in\nan XML document: CData , ProcessingInstruction , Declaration , and Doctype . Just like Comment , these classes\nare subclasses of NavigableString that add something extra to the\nstring. Hereâs an example that replaces the comment with a CDATA\nblock: from bs4 import CData cdata = CData ( \"A CDATA block\" ) comment . replace_with ( cdata ) print ( soup . b . prettify ()) # <b> # <![CDATA[A CDATA block]]> # </b> Navigating the tree Â¶ Hereâs the âThree sistersâ HTML document again: html_doc = \"\"\" <html><head><title>The Dormouse's story</title></head> <body> <p class=\"title\"><b>The Dormouse's story</b></p> <p class=\"story\">Once upon a time there were three little sisters; and their names were <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>, <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and <a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>; and they lived at the bottom of a well.</p> <p class=\"story\">...</p> \"\"\" from bs4 import BeautifulSoup soup = BeautifulSoup ( html_doc , 'html.parser' ) Iâll use this as an example to show you how to move from one part of\na document to another. Going down Â¶ Tags may contain strings and other tags. These elements are the tagâs children . Beautiful Soup provides a lot of different attributes for\nnavigating and iterating over a tagâs children. Note that Beautiful Soup strings donât support any of these\nattributes, because a string canât have children. Navigating using tag names Â¶ The simplest way to navigate the parse tree is to say the name of the\ntag you want. If you want the <head> tag, just say soup.head : soup . head # <head><title>The Dormouse's story</title></head> soup . title # <title>The Dormouse's story</title> You can do use this trick again and again to zoom in on a certain part\nof the parse tree. This code gets the first <b> tag beneath the <body> tag: soup . body . b # <b>The Dormouse's story</b> Using a tag name as an attribute will give you only the first tag by that\nname: soup . a # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> If you need to get all the <a> tags, or anything more complicated\nthan the first tag with a certain name, youâll need to use one of the\nmethods described in Searching the tree , such as find_all() : soup . find_all ( 'a' ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] .contents and .children Â¶ A tagâs children are available in a list called .contents : head_tag = soup . head head_tag # <head><title>The Dormouse's story</title></head> head_tag . contents [ < title > The Dormouse 's story</title>] title_tag = head_tag . contents [ 0 ] title_tag # <title>The Dormouse's story</title> title_tag . contents # [u'The Dormouse's story'] The BeautifulSoup object itself has children. In this case, the\n<html> tag is the child of the BeautifulSoup object.: len ( soup . contents ) # 1 soup . contents [ 0 ] . name # u'html' A string does not have .contents , because it canât contain\nanything: text = title_tag . contents [ 0 ] text . contents # AttributeError: 'NavigableString' object has no attribute 'contents' Instead of getting them as a list, you can iterate over a tagâs\nchildren using the .children generator: for child in title_tag . children : print ( child ) # The Dormouse's story .descendants Â¶ The .contents and .children attributes only consider a tagâs direct children. For instance, the <head> tag has a single direct\nchildâthe <title> tag: head_tag . contents # [<title>The Dormouse's story</title>] But the <title> tag itself has a child: the string âThe Dormouseâs\nstoryâ. Thereâs a sense in which that string is also a child of the\n<head> tag. The .descendants attribute lets you iterate over all of a tagâs children, recursively: its direct children, the children of\nits direct children, and so on: for child in head_tag . descendants : print ( child ) # <title>The Dormouse's story</title> # The Dormouse's story The <head> tag has only one child, but it has two descendants: the\n<title> tag and the <title> tagâs child. The BeautifulSoup object\nonly has one direct child (the <html> tag), but it has a whole lot of\ndescendants: len ( list ( soup . children )) # 1 len ( list ( soup . descendants )) # 25 .string Â¶ If a tag has only one child, and that child is a NavigableString ,\nthe child is made available as .string : title_tag . string # u'The Dormouse's story' If a tagâs only child is another tag, and that tag has a .string , then the parent tag is considered to have the same .string as its child: head_tag . contents # [<title>The Dormouse's story</title>] head_tag . string # u'The Dormouse's story' If a tag contains more than one thing, then itâs not clear what .string should refer to, so .string is defined to be None : print ( soup . html . string ) # None .strings and stripped_strings Â¶ If thereâs more than one thing inside a tag, you can still look at\njust the strings. Use the .strings generator: for string in soup . strings : print ( repr ( string )) # u\"The Dormouse's story\" # u'\\n\\n' # u\"The Dormouse's story\" # u'\\n\\n' # u'Once upon a time there were three little sisters; and their names were\\n' # u'Elsie' # u',\\n' # u'Lacie' # u' and\\n' # u'Tillie' # u';\\nand they lived at the bottom of a well.' # u'\\n\\n' # u'...' # u'\\n' These strings tend to have a lot of extra whitespace, which you can\nremove by using the .stripped_strings generator instead: for string in soup . stripped_strings : print ( repr ( string )) # u\"The Dormouse's story\" # u\"The Dormouse's story\" # u'Once upon a time there were three little sisters; and their names were' # u'Elsie' # u',' # u'Lacie' # u'and' # u'Tillie' # u';\\nand they lived at the bottom of a well.' # u'...' Here, strings consisting entirely of whitespace are ignored, and\nwhitespace at the beginning and end of strings is removed. Going up Â¶ Continuing the âfamily treeâ analogy, every tag and every string has a parent : the tag that contains it. .parent Â¶ You can access an elementâs parent with the .parent attribute. In\nthe example âthree sistersâ document, the <head> tag is the parent\nof the <title> tag: title_tag = soup . title title_tag # <title>The Dormouse's story</title> title_tag . parent # <head><title>The Dormouse's story</title></head> The title string itself has a parent: the <title> tag that contains\nit: title_tag . string . parent # <title>The Dormouse's story</title> The parent of a top-level tag like <html> is the BeautifulSoup object\nitself: html_tag = soup . html type ( html_tag . parent ) # <class 'bs4.BeautifulSoup'> And the .parent of a BeautifulSoup object is defined as None: print ( soup . parent ) # None .parents Â¶ You can iterate over all of an elementâs parents with .parents . This example uses .parents to travel from an <a> tag\nburied deep within the document, to the very top of the document: link = soup . a link # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> for parent in link . parents : if parent is None : print ( parent ) else : print ( parent . name ) # p # body # html # [document] # None Going sideways Â¶ Consider a simple document like this: sibling_soup = BeautifulSoup ( \"<a><b>text1</b><c>text2</c></b></a>\" ) print ( sibling_soup . prettify ()) # <html> # <body> # <a> # <b> # text1 # </b> # <c> # text2 # </c> # </a> # </body> # </html> The <b> tag and the <c> tag are at the same level: theyâre both direct\nchildren of the same tag. We call them siblings . When a document is\npretty-printed, siblings show up at the same indentation level. You\ncan also use this relationship in the code you write. .next_sibling and .previous_sibling Â¶ You can use .next_sibling and .previous_sibling to navigate\nbetween page elements that are on the same level of the parse tree: sibling_soup . b . next_sibling # <c>text2</c> sibling_soup . c . previous_sibling # <b>text1</b> The <b> tag has a .next_sibling , but no .previous_sibling ,\nbecause thereâs nothing before the <b> tag on the same level of the\ntree . For the same reason, the <c> tag has a .previous_sibling but no .next_sibling : print ( sibling_soup . b . previous_sibling ) # None print ( sibling_soup . c . next_sibling ) # None The strings âtext1â and âtext2â are not siblings, because they donât\nhave the same parent: sibling_soup . b . string # u'text1' print ( sibling_soup . b . string . next_sibling ) # None In real documents, the .next_sibling or .previous_sibling of a\ntag will usually be a string containing whitespace. Going back to the\nâthree sistersâ document: < a href = \"http://example.com/elsie\" class = \"sister\" id = \"link1\" > Elsie </ a > < a href = \"http://example.com/lacie\" class = \"sister\" id = \"link2\" > Lacie </ a > < a href = \"http://example.com/tillie\" class = \"sister\" id = \"link3\" > Tillie </ a > You might think that the .next_sibling of the first <a> tag would\nbe the second <a> tag. But actually, itâs a string: the comma and\nnewline that separate the first <a> tag from the second: link = soup . a link # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> link . next_sibling # u',\\n' The second <a> tag is actually the .next_sibling of the comma: link . next_sibling . next_sibling # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> .next_siblings and .previous_siblings Â¶ You can iterate over a tagâs siblings with .next_siblings or .previous_siblings : for sibling in soup . a . next_siblings : print ( repr ( sibling )) # u',\\n' # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> # u' and\\n' # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a> # u'; and they lived at the bottom of a well.' # None for sibling in soup . find ( id = \"link3\" ) . previous_siblings : print ( repr ( sibling )) # ' and\\n' # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> # u',\\n' # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> # u'Once upon a time there were three little sisters; and their names were\\n' # None Going back and forth Â¶ Take a look at the beginning of the âthree sistersâ document: < html >< head >< title > The Dormouse 's story</title></head> < p class = \"title\" >< b > The Dormouse 's story</b></p> An HTML parser takes this string of characters and turns it into a\nseries of events: âopen an <html> tagâ, âopen a <head> tagâ, âopen a\n<title> tagâ, âadd a stringâ, âclose the <title> tagâ, âopen a <p>\ntagâ, and so on. Beautiful Soup offers tools for reconstructing the\ninitial parse of the document. .next_element and .previous_element Â¶ The .next_element attribute of a string or tag points to whatever\nwas parsed immediately afterwards. It might be the same as .next_sibling , but itâs usually drastically different. Hereâs the final <a> tag in the âthree sistersâ document. Its .next_sibling is a string: the conclusion of the sentence that was\ninterrupted by the start of the <a> tag.: last_a_tag = soup . find ( \"a\" , id = \"link3\" ) last_a_tag # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a> last_a_tag . next_sibling # '; and they lived at the bottom of a well.' But the .next_element of that <a> tag, the thing that was parsed\nimmediately after the <a> tag, is not the rest of that sentence:\nitâs the word âTillieâ: last_a_tag . next_element # u'Tillie' Thatâs because in the original markup, the word âTillieâ appeared\nbefore that semicolon. The parser encountered an <a> tag, then the\nword âTillieâ, then the closing </a> tag, then the semicolon and rest of\nthe sentence. The semicolon is on the same level as the <a> tag, but the\nword âTillieâ was encountered first. The .previous_element attribute is the exact opposite of .next_element . It points to whatever element was parsed\nimmediately before this one: last_a_tag . previous_element # u' and\\n' last_a_tag . previous_element . next_element # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a> .next_elements and .previous_elements Â¶ You should get the idea by now. You can use these iterators to move\nforward or backward in the document as it was parsed: for element in last_a_tag . next_elements : print ( repr ( element )) # u'Tillie' # u';\\nand they lived at the bottom of a well.' # u'\\n\\n' # <p class=\"story\">...</p> # u'...' # u'\\n' # None Searching the tree Â¶ Beautiful Soup defines a lot of methods for searching the parse tree,\nbut theyâre all very similar. Iâm going to spend a lot of time explaining\nthe two most popular methods: find() and find_all() . The other\nmethods take almost exactly the same arguments, so Iâll just cover\nthem briefly. Once again, Iâll be using the âthree sistersâ document as an example: html_doc = \"\"\" <html><head><title>The Dormouse's story</title></head> <body> <p class=\"title\"><b>The Dormouse's story</b></p> <p class=\"story\">Once upon a time there were three little sisters; and their names were <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>, <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and <a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>; and they lived at the bottom of a well.</p> <p class=\"story\">...</p> \"\"\" from bs4 import BeautifulSoup soup = BeautifulSoup ( html_doc , 'html.parser' ) By passing in a filter to an argument like find_all() , you can\nzoom in on the parts of the document youâre interested in. Kinds of filters Â¶ Before talking in detail about find_all() and similar methods, I\nwant to show examples of different filters you can pass into these\nmethods. These filters show up again and again, throughout the\nsearch API. You can use them to filter based on a tagâs name,\non its attributes, on the text of a string, or on some combination of\nthese. A string Â¶ The simplest filter is a string. Pass a string to a search method and\nBeautiful Soup will perform a match against that exact string. This\ncode finds all the <b> tags in the document: soup . find_all ( 'b' ) # [<b>The Dormouse's story</b>] If you pass in a byte string, Beautiful Soup will assume the string is\nencoded as UTF-8. You can avoid this by passing in a Unicode string instead. A regular expression Â¶ If you pass in a regular expression object, Beautiful Soup will filter\nagainst that regular expression using its search() method. This code\nfinds all the tags whose names start with the letter âbâ; in this\ncase, the <body> tag and the <b> tag: import re for tag in soup . find_all ( re . compile ( \"^b\" )): print ( tag . name ) # body # b This code finds all the tags whose names contain the letter âtâ: for tag in soup . find_all ( re . compile ( \"t\" )): print ( tag . name ) # html # title A list Â¶ If you pass in a list, Beautiful Soup will allow a string match\nagainst any item in that list. This code finds all the <a> tags and all the <b> tags: soup . find_all ([ \"a\" , \"b\" ]) # [<b>The Dormouse's story</b>, # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] True Â¶ The value True matches everything it can. This code finds all the tags in the document, but none of the text strings: for tag in soup . find_all ( True ): print ( tag . name ) # html # head # title # body # p # b # p # a # a # a # p A function Â¶ If none of the other matches work for you, define a function that\ntakes an element as its only argument. The function should return True if the argument matches, and False otherwise. Hereâs a function that returns True if a tag defines the âclassâ\nattribute but doesnât define the âidâ attribute: def has_class_but_no_id ( tag ): return tag . has_attr ( 'class' ) and not tag . has_attr ( 'id' ) Pass this function into find_all() and youâll pick up all the <p>\ntags: soup . find_all ( has_class_but_no_id ) # [<p class=\"title\"><b>The Dormouse's story</b></p>, # <p class=\"story\">Once upon a time there were...</p>, # <p class=\"story\">...</p>] This function only picks up the <p> tags. It doesnât pick up the <a>\ntags, because those tags define both âclassâ and âidâ. It doesnât pick\nup tags like <html> and <title>, because those tags donât define\nâclassâ. If you pass in a function to filter on a specific attribute like href , the argument passed into the function will be the attribute\nvalue, not the whole tag. Hereâs a function that finds all a tags\nwhose href attribute does not match a regular expression: def not_lacie ( href ): return href and not re . compile ( \"lacie\" ) . search ( href ) soup . find_all ( href = not_lacie ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] The function can be as complicated as you need it to be. Hereâs a\nfunction that returns True if a tag is surrounded by string\nobjects: from bs4 import NavigableString def surrounded_by_strings ( tag ): return ( isinstance ( tag . next_element , NavigableString ) and isinstance ( tag . previous_element , NavigableString )) for tag in soup . find_all ( surrounded_by_strings ): print tag . name # p # a # a # a # p Now weâre ready to look at the search methods in detail. find_all() Â¶ Signature: find_all( name , attrs , recursive , string , limit , **kwargs ) The find_all() method looks through a tagâs descendants and\nretrieves all descendants that match your filters. I gave several\nexamples in Kinds of filters , but here are a few more: soup . find_all ( \"title\" ) # [<title>The Dormouse's story</title>] soup . find_all ( \"p\" , \"title\" ) # [<p class=\"title\"><b>The Dormouse's story</b></p>] soup . find_all ( \"a\" ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] soup . find_all ( id = \"link2\" ) # [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>] import re soup . find ( string = re . compile ( \"sisters\" )) # u'Once upon a time there were three little sisters; and their names were\\n' Some of these should look familiar, but others are new. What does it\nmean to pass in a value for string , or id ? Why does find_all(\"p\", \"title\") find a <p> tag with the CSS class âtitleâ?\nLetâs look at the arguments to find_all() . The name argument Â¶ Pass in a value for name and youâll tell Beautiful Soup to only\nconsider tags with certain names. Text strings will be ignored, as\nwill tags whose names that donât match. This is the simplest usage: soup . find_all ( \"title\" ) # [<title>The Dormouse's story</title>] Recall from Kinds of filters that the value to name can be a\nstring , a regular expression , a list , a function , or the value\nTrue . The keyword arguments Â¶ Any argument thatâs not recognized will be turned into a filter on one\nof a tagâs attributes. If you pass in a value for an argument called id ,\nBeautiful Soup will filter against each tagâs âidâ attribute: soup . find_all ( id = 'link2' ) # [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>] If you pass in a value for href , Beautiful Soup will filter\nagainst each tagâs âhrefâ attribute: soup . find_all ( href = re . compile ( \"elsie\" )) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>] You can filter an attribute based on a string , a regular\nexpression , a list , a function , or the value True . This code finds all tags whose id attribute has a value,\nregardless of what the value is: soup . find_all ( id = True ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] You can filter multiple attributes at once by passing in more than one\nkeyword argument: soup . find_all ( href = re . compile ( \"elsie\" ), id = 'link1' ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">three</a>] Some attributes, like the data-* attributes in HTML 5, have names that\ncanât be used as the names of keyword arguments: data_soup = BeautifulSoup ( '<div data-foo=\"value\">foo!</div>' ) data_soup . find_all ( data - foo = \"value\" ) # SyntaxError: keyword can't be an expression You can use these attributes in searches by putting them into a\ndictionary and passing the dictionary into find_all() as the attrs argument: data_soup . find_all ( attrs = { \"data-foo\" : \"value\" }) # [<div data-foo=\"value\">foo!</div>] You canât use a keyword argument to search for HTMLâs ânameâ element,\nbecause Beautiful Soup uses the name argument to contain the name\nof the tag itself. Instead, you can give a value to ânameâ in the attrs argument: name_soup = BeautifulSoup ( '<input name=\"email\"/>' ) name_soup . find_all ( name = \"email\" ) # [] name_soup . find_all ( attrs = { \"name\" : \"email\" }) # [<input name=\"email\"/>] Searching by CSS class Â¶ Itâs very useful to search for a tag that has a certain CSS class, but\nthe name of the CSS attribute, âclassâ, is a reserved word in\nPython. Using class as a keyword argument will give you a syntax\nerror. As of Beautiful Soup 4.1.2, you can search by CSS class using\nthe keyword argument class_ : soup . find_all ( \"a\" , class_ = \"sister\" ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] As with any keyword argument, you can pass class_ a string, a regular\nexpression, a function, or True : soup . find_all ( class_ = re . compile ( \"itl\" )) # [<p class=\"title\"><b>The Dormouse's story</b></p>] def has_six_characters ( css_class ): return css_class is not None and len ( css_class ) == 6 soup . find_all ( class_ = has_six_characters ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] Remember that a single tag can have multiple\nvalues for its âclassâ attribute. When you search for a tag that\nmatches a certain CSS class, youâre matching against any of its CSS\nclasses: css_soup = BeautifulSoup ( '<p class=\"body strikeout\"></p>' ) css_soup . find_all ( \"p\" , class_ = \"strikeout\" ) # [<p class=\"body strikeout\"></p>] css_soup . find_all ( \"p\" , class_ = \"body\" ) # [<p class=\"body strikeout\"></p>] You can also search for the exact string value of the class attribute: css_soup . find_all ( \"p\" , class_ = \"body strikeout\" ) # [<p class=\"body strikeout\"></p>] But searching for variants of the string value wonât work: css_soup . find_all ( \"p\" , class_ = \"strikeout body\" ) # [] If you want to search for tags that match two or more CSS classes, you\nshould use a CSS selector: css_soup . select ( \"p.strikeout.body\" ) # [<p class=\"body strikeout\"></p>] In older versions of Beautiful Soup, which donât have the class_ shortcut, you can use the attrs trick mentioned above. Create a\ndictionary whose value for âclassâ is the string (or regular\nexpression, or whatever) you want to search for: soup . find_all ( \"a\" , attrs = { \"class\" : \"sister\" }) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] The string argument Â¶ With string you can search for strings instead of tags. As with name and the keyword arguments, you can pass in a string , a\nregular expression , a list , a function , or the value True .\nHere are some examples: soup . find_all ( string = \"Elsie\" ) # [u'Elsie'] soup . find_all ( string = [ \"Tillie\" , \"Elsie\" , \"Lacie\" ]) # [u'Elsie', u'Lacie', u'Tillie'] soup . find_all ( string = re . compile ( \"Dormouse\" )) [ u \"The Dormouse's story\" , u \"The Dormouse's story\" ] def is_the_only_string_within_a_tag ( s ): \"\"\"Return True if this string is the only child of its parent tag.\"\"\" return ( s == s . parent . string ) soup . find_all ( string = is_the_only_string_within_a_tag ) # [u\"The Dormouse's story\", u\"The Dormouse's story\", u'Elsie', u'Lacie', u'Tillie', u'...'] Although string is for finding strings, you can combine it with\narguments that find tags: Beautiful Soup will find all tags whose .string matches your value for string . This code finds the <a>\ntags whose .string is âElsieâ: soup . find_all ( \"a\" , string = \"Elsie\" ) # [<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>] The string argument is new in Beautiful Soup 4.4.0. In earlier\nversions it was called text : soup . find_all ( \"a\" , text = \"Elsie\" ) # [<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>] The limit argument Â¶ find_all() returns all the tags and strings that match your\nfilters. This can take a while if the document is large. If you donât\nneed all the results, you can pass in a number for limit . This\nworks just like the LIMIT keyword in SQL. It tells Beautiful Soup to\nstop gathering results after itâs found a certain number. There are three links in the âthree sistersâ document, but this code\nonly finds the first two: soup . find_all ( \"a\" , limit = 2 ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>] The recursive argument Â¶ If you call mytag.find_all() , Beautiful Soup will examine all the\ndescendants of mytag : its children, its childrenâs children, and\nso on. If you only want Beautiful Soup to consider direct children,\nyou can pass in recursive=False . See the difference here: soup . html . find_all ( \"title\" ) # [<title>The Dormouse's story</title>] soup . html . find_all ( \"title\" , recursive = False ) # [] Hereâs that part of the document: < html > < head > < title > The Dormouse 's story </ title > </ head > ... The <title> tag is beneath the <html> tag, but itâs not directly beneath the <html> tag: the <head> tag is in the way. Beautiful Soup\nfinds the <title> tag when itâs allowed to look at all descendants of\nthe <html> tag, but when recursive=False restricts it to the\n<html> tagâs immediate children, it finds nothing. Beautiful Soup offers a lot of tree-searching methods (covered below),\nand they mostly take the same arguments as find_all() : name , attrs , string , limit , and the keyword arguments. But the recursive argument is different: find_all() and find() are\nthe only methods that support it. Passing recursive=False into a\nmethod like find_parents() wouldnât be very useful. Calling a tag is like calling find_all() Â¶ Because find_all() is the most popular method in the Beautiful\nSoup search API, you can use a shortcut for it. If you treat the BeautifulSoup object or a Tag object as though it were a\nfunction, then itâs the same as calling find_all() on that\nobject. These two lines of code are equivalent: soup . find_all ( \"a\" ) soup ( \"a\" ) These two lines are also equivalent: soup . title . find_all ( string = True ) soup . title ( string = True ) find() Â¶ Signature: find( name , attrs , recursive , string , **kwargs ) The find_all() method scans the entire document looking for\nresults, but sometimes you only want to find one result. If you know a\ndocument only has one <body> tag, itâs a waste of time to scan the\nentire document looking for more. Rather than passing in limit=1 every time you call find_all , you can use the find() method. These two lines of code are nearly equivalent: soup . find_all ( 'title' , limit = 1 ) # [<title>The Dormouse's story</title>] soup . find ( 'title' ) # <title>The Dormouse's story</title> The only difference is that find_all() returns a list containing\nthe single result, and find() just returns the result. If find_all() canât find anything, it returns an empty list. If find() canât find anything, it returns None : print ( soup . find ( \"nosuchtag\" )) # None Remember the soup.head.title trick from Navigating using tag\nnames ? That trick works by repeatedly calling find() : soup . head . title # <title>The Dormouse's story</title> soup . find ( \"head\" ) . find ( \"title\" ) # <title>The Dormouse's story</title> find_parents() and find_parent() Â¶ Signature: find_parents( name , attrs , string , limit , **kwargs ) Signature: find_parent( name , attrs , string , **kwargs ) I spent a lot of time above covering find_all() and find() . The Beautiful Soup API defines ten other methods for\nsearching the tree, but donât be afraid. Five of these methods are\nbasically the same as find_all() , and the other five are basically\nthe same as find() . The only differences are in what parts of the\ntree they search. First letâs consider find_parents() and find_parent() . Remember that find_all() and find() work\ntheir way down the tree, looking at tagâs descendants. These methods\ndo the opposite: they work their way up the tree, looking at a tagâs\n(or a stringâs) parents. Letâs try them out, starting from a string\nburied deep in the âthree daughtersâ document: a_string = soup . find ( string = \"Lacie\" ) a_string # u'Lacie' a_string . find_parents ( \"a\" ) # [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>] a_string . find_parent ( \"p\" ) # <p class=\"story\">Once upon a time there were three little sisters; and their names were # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>; # and they lived at the bottom of a well.</p> a_string . find_parents ( \"p\" , class = \"title\" ) # [] One of the three <a> tags is the direct parent of the string in\nquestion, so our search finds it. One of the three <p> tags is an\nindirect parent of the string, and our search finds that as\nwell. Thereâs a <p> tag with the CSS class âtitleâ somewhere in the\ndocument, but itâs not one of this stringâs parents, so we canât find\nit with find_parents() . You may have made the connection between find_parent() and find_parents() , and the .parent and .parents attributes\nmentioned earlier. The connection is very strong. These search methods\nactually use .parents to iterate over all the parents, and check\neach one against the provided filter to see if it matches. find_next_siblings() and find_next_sibling() Â¶ Signature: find_next_siblings( name , attrs , string , limit , **kwargs ) Signature: find_next_sibling( name , attrs , string , **kwargs ) These methods use .next_siblings to\niterate over the rest of an elementâs siblings in the tree. The find_next_siblings() method returns all the siblings that match,\nand find_next_sibling() only returns the first one: first_link = soup . a first_link # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> first_link . find_next_siblings ( \"a\" ) # [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] first_story_paragraph = soup . find ( \"p\" , \"story\" ) first_story_paragraph . find_next_sibling ( \"p\" ) # <p class=\"story\">...</p> find_previous_siblings() and find_previous_sibling() Â¶ Signature: find_previous_siblings( name , attrs , string , limit , **kwargs ) Signature: find_previous_sibling( name , attrs , string , **kwargs ) These methods use .previous_siblings to iterate over an elementâs\nsiblings that precede it in the tree. The find_previous_siblings() method returns all the siblings that match, and find_previous_sibling() only returns the first one: last_link = soup . find ( \"a\" , id = \"link3\" ) last_link # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a> last_link . find_previous_siblings ( \"a\" ) # [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>] first_story_paragraph = soup . find ( \"p\" , \"story\" ) first_story_paragraph . find_previous_sibling ( \"p\" ) # <p class=\"title\"><b>The Dormouse's story</b></p> find_all_next() and find_next() Â¶ Signature: find_all_next( name , attrs , string , limit , **kwargs ) Signature: find_next( name , attrs , string , **kwargs ) These methods use .next_elements to\niterate over whatever tags and strings that come after it in the\ndocument. The find_all_next() method returns all matches, and find_next() only returns the first match: first_link = soup . a first_link # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> first_link . find_all_next ( string = True ) # [u'Elsie', u',\\n', u'Lacie', u' and\\n', u'Tillie', # u';\\nand they lived at the bottom of a well.', u'\\n\\n', u'...', u'\\n'] first_link . find_next ( \"p\" ) # <p class=\"story\">...</p> In the first example, the string âElsieâ showed up, even though it was\ncontained within the <a> tag we started from. In the second example,\nthe last <p> tag in the document showed up, even though itâs not in\nthe same part of the tree as the <a> tag we started from. For these\nmethods, all that matters is that an element match the filter, and\nshow up later in the document than the starting element. find_all_previous() and find_previous() Â¶ Signature: find_all_previous( name , attrs , string , limit , **kwargs ) Signature: find_previous( name , attrs , string , **kwargs ) These methods use .previous_elements to\niterate over the tags and strings that came before it in the\ndocument. The find_all_previous() method returns all matches, and find_previous() only returns the first match: first_link = soup . a first_link # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> first_link . find_all_previous ( \"p\" ) # [<p class=\"story\">Once upon a time there were three little sisters; ...</p>, # <p class=\"title\"><b>The Dormouse's story</b></p>] first_link . find_previous ( \"title\" ) # <title>The Dormouse's story</title> The call to find_all_previous(\"p\") found the first paragraph in\nthe document (the one with class=âtitleâ), but it also finds the\nsecond paragraph, the <p> tag that contains the <a> tag we started\nwith. This shouldnât be too surprising: weâre looking at all the tags\nthat show up earlier in the document than the one we started with. A\n<p> tag that contains an <a> tag must have shown up before the <a>\ntag it contains. CSS selectors Â¶ As of version 4.7.0, Beautiful Soup supports most CSS4 selectors via\nthe SoupSieve project. If you installed Beautiful Soup through pip , SoupSieve\nwas installed at the same time, so you donât have to do anything extra. BeautifulSoup has a .select() method which uses SoupSieve to\nrun a CSS selector against a parsed document and return all the\nmatching elements. Tag has a similar method which runs a CSS\nselector against the contents of a single tag. (Earlier versions of Beautiful Soup also have the .select() method, but only the most commonly-used CSS selectors are supported.) The SoupSieve documentation lists all the currently\nsupported CSS selectors, but here are some of the basics: You can find tags: soup . select ( \"title\" ) # [<title>The Dormouse's story</title>] soup . select ( \"p:nth-of-type(3)\" ) # [<p class=\"story\">...</p>] Find tags beneath other tags: soup . select ( \"body a\" ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] soup . select ( \"html head title\" ) # [<title>The Dormouse's story</title>] Find tags directly beneath other tags: soup . select ( \"head > title\" ) # [<title>The Dormouse's story</title>] soup . select ( \"p > a\" ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] soup . select ( \"p > a:nth-of-type(2)\" ) # [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>] soup . select ( \"p > #link1\" ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>] soup . select ( \"body > a\" ) # [] Find the siblings of tags: soup . select ( \"#link1 ~ .sister\" ) # [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] soup . select ( \"#link1 + .sister\" ) # [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>] Find tags by CSS class: soup . select ( \".sister\" ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] soup . select ( \"[class~=sister]\" ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] Find tags by ID: soup . select ( \"#link1\" ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>] soup . select ( \"a#link2\" ) # [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>] Find tags that match any selector from a list of selectors: soup . select ( \"#link1,#link2\" ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>] Test for the existence of an attribute: soup . select ( 'a[href]' ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] Find tags by attribute value: soup . select ( 'a[href=\"http://example.com/elsie\"]' ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>] soup . select ( 'a[href^=\"http://example.com/\"]' ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] soup . select ( 'a[href$=\"tillie\"]' ) # [<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>] soup . select ( 'a[href*=\".com/el\"]' ) # [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>] Thereâs also a method called select_one() , which finds only the\nfirst tag that matches a selector: soup . select_one ( \".sister\" ) # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a> If youâve parsed XML that defines namespaces, you can use them in CSS\nselectors.: from bs4 import BeautifulSoup xml = \"\"\"<tag xmlns:ns1=\"http://namespace1/\" xmlns:ns2=\"http://namespace2/\"> <ns1:child>I'm in namespace 1</ns1:child> <ns2:child>I'm in namespace 2</ns2:child> </tag> \"\"\" soup = BeautifulSoup ( xml , \"xml\" ) soup . select ( \"child\" ) # [<ns1:child>I'm in namespace 1</ns1:child>, <ns2:child>I'm in namespace 2</ns2:child>] soup . select ( \"ns1|child\" , namespaces = namespaces ) # [<ns1:child>I'm in namespace 1</ns1:child>] When handling a CSS selector that uses namespaces, Beautiful Soup\nuses the namespace abbreviations it found when parsing the\ndocument. You can override this by passing in your own dictionary of\nabbreviations: namespaces = dict ( first = \"http://namespace1/\" , second = \"http://namespace2/\" ) soup . select ( \"second|child\" , namespaces = namespaces ) # [<ns1:child>I'm in namespace 2</ns1:child>] All this CSS selector stuff is a convenience for people who already\nknow the CSS selector syntax. You can do all of this with the\nBeautiful Soup API. And if CSS selectors are all you need, you should\nparse the document with lxml: itâs a lot faster. But this lets you combine CSS selectors with the Beautiful Soup API. Modifying the tree Â¶ Beautiful Soupâs main strength is in searching the parse tree, but you\ncan also modify the tree and write your changes as a new HTML or XML\ndocument. Changing tag names and attributes Â¶ I covered this earlier, in Attributes , but it bears repeating. You\ncan rename a tag, change the values of its attributes, add new\nattributes, and delete attributes: soup = BeautifulSoup ( '<b class=\"boldest\">Extremely bold</b>' ) tag = soup . b tag . name = \"blockquote\" tag [ 'class' ] = 'verybold' tag [ 'id' ] = 1 tag # <blockquote class=\"verybold\" id=\"1\">Extremely bold</blockquote> del tag [ 'class' ] del tag [ 'id' ] tag # <blockquote>Extremely bold</blockquote> Modifying .string Â¶ If you set a tagâs .string attribute to a new string, the tagâs contents are\nreplaced with that string: markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>' soup = BeautifulSoup ( markup ) tag = soup . a tag . string = \"New link text.\" tag # <a href=\"http://example.com/\">New link text.</a> Be careful: if the tag contained other tags, they and all their\ncontents will be destroyed. append() Â¶ You can add to a tagâs contents with Tag.append() . It works just\nlike calling .append() on a Python list: soup = BeautifulSoup ( \"<a>Foo</a>\" ) soup . a . append ( \"Bar\" ) soup # <html><head></head><body><a>FooBar</a></body></html> soup . a . contents # [u'Foo', u'Bar'] extend() Â¶ Starting in Beautiful Soup 4.7.0, Tag also supports a method\ncalled .extend() , which works just like calling .extend() on a\nPython list: soup = BeautifulSoup ( \"<a>Soup</a>\" ) soup . a . extend ([ \"'s\" , \" \" , \"on\" ]) soup # <html><head></head><body><a>Soup's on</a></body></html> soup . a . contents # [u'Soup', u''s', u' ', u'on'] NavigableString() and .new_tag() Â¶ If you need to add a string to a document, no problemâyou can pass a\nPython string in to append() , or you can call the NavigableString constructor: soup = BeautifulSoup ( \"<b></b>\" ) tag = soup . b tag . append ( \"Hello\" ) new_string = NavigableString ( \" there\" ) tag . append ( new_string ) tag # <b>Hello there.</b> tag . contents # [u'Hello', u' there'] If you want to create a comment or some other subclass of NavigableString , just call the constructor: from bs4 import Comment new_comment = Comment ( \"Nice to see you.\" ) tag . append ( new_comment ) tag # <b>Hello there<!--Nice to see you.--></b> tag . contents # [u'Hello', u' there', u'Nice to see you.'] (This is a new feature in Beautiful Soup 4.4.0.) What if you need to create a whole new tag? The best solution is to\ncall the factory method BeautifulSoup.new_tag() : soup = BeautifulSoup ( \"<b></b>\" ) original_tag = soup . b new_tag = soup . new_tag ( \"a\" , href = \"http://www.example.com\" ) original_tag . append ( new_tag ) original_tag # <b><a href=\"http://www.example.com\"></a></b> new_tag . string = \"Link text.\" original_tag # <b><a href=\"http://www.example.com\">Link text.</a></b> Only the first argument, the tag name, is required. insert() Â¶ Tag.insert() is just like Tag.append() , except the new element\ndoesnât necessarily go at the end of its parentâs .contents . Itâll be inserted at whatever numeric position you\nsay. It works just like .insert() on a Python list: markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>' soup = BeautifulSoup ( markup ) tag = soup . a tag . insert ( 1 , \"but did not endorse \" ) tag # <a href=\"http://example.com/\">I linked to but did not endorse <i>example.com</i></a> tag . contents # [u'I linked to ', u'but did not endorse', <i>example.com</i>] insert_before() and insert_after() Â¶ The insert_before() method inserts tags or strings immediately\nbefore something else in the parse tree: soup = BeautifulSoup ( \"<b>stop</b>\" ) tag = soup . new_tag ( \"i\" ) tag . string = \"Don't\" soup . b . string . insert_before ( tag ) soup . b # <b><i>Don't</i>stop</b> The insert_after() method inserts tags or strings immediately\nfollowing something else in the parse tree: div = soup . new_tag ( 'div' ) div . string = 'ever' soup . b . i . insert_after ( \" you \" , div ) soup . b # <b><i>Don't</i> you <div>ever</div> stop</b> soup . b . contents # [<i>Don't</i>, u' you', <div>ever</div>, u'stop'] clear() Â¶ Tag.clear() removes the contents of a tag: markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>' soup = BeautifulSoup ( markup ) tag = soup . a tag . clear () tag # <a href=\"http://example.com/\"></a> extract() Â¶ PageElement.extract() removes a tag or string from the tree. It\nreturns the tag or string that was extracted: markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>' soup = BeautifulSoup ( markup ) a_tag = soup . a i_tag = soup . i . extract () a_tag # <a href=\"http://example.com/\">I linked to</a> i_tag # <i>example.com</i> print ( i_tag . parent ) None At this point you effectively have two parse trees: one rooted at the BeautifulSoup object you used to parse the document, and one rooted\nat the tag that was extracted. You can go on to call extract on\na child of the element you extracted: my_string = i_tag . string . extract () my_string # u'example.com' print ( my_string . parent ) # None i_tag # <i></i> decompose() Â¶ Tag.decompose() removes a tag from the tree, then completely\ndestroys it and its contents : markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>' soup = BeautifulSoup ( markup ) a_tag = soup . a soup . i . decompose () a_tag # <a href=\"http://example.com/\">I linked to</a> replace_with() Â¶ PageElement.replace_with() removes a tag or string from the tree,\nand replaces it with the tag or string of your choice: markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>' soup = BeautifulSoup ( markup ) a_tag = soup . a new_tag = soup . new_tag ( \"b\" ) new_tag . string = \"example.net\" a_tag . i . replace_with ( new_tag ) a_tag # <a href=\"http://example.com/\">I linked to <b>example.net</b></a> replace_with() returns the tag or string that was replaced, so\nthat you can examine it or add it back to another part of the tree. wrap() Â¶ PageElement.wrap() wraps an element in the tag you specify. It\nreturns the new wrapper: soup = BeautifulSoup ( \"<p>I wish I was bold.</p>\" ) soup . p . string . wrap ( soup . new_tag ( \"b\" )) # <b>I wish I was bold.</b> soup . p . wrap ( soup . new_tag ( \"div\" ) # <div><p><b>I wish I was bold.</b></p></div> This method is new in Beautiful Soup 4.0.5. unwrap() Â¶ Tag.unwrap() is the opposite of wrap() . It replaces a tag with\nwhateverâs inside that tag. Itâs good for stripping out markup: markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>' soup = BeautifulSoup ( markup ) a_tag = soup . a a_tag . i . unwrap () a_tag # <a href=\"http://example.com/\">I linked to example.com</a> Like replace_with() , unwrap() returns the tag\nthat was replaced. smooth() Â¶ After calling a bunch of methods that modify the parse tree, you may end up with two or more NavigableString objects next to each other. Beautiful Soup doesnât have any problems with this, but since it canât happen in a freshly parsed document, you might not expect behavior like the following: soup = BeautifulSoup ( \"<p>A one</p>\" ) soup . p . append ( \", a two\" ) soup . p . contents # [u'A one', u', a two'] print ( soup . p . encode ()) # <p>A one, a two</p> print ( soup . p . prettify ()) # <p> # A one # , a two # </p> You can call Tag.smooth() to clean up the parse tree by consolidating adjacent strings: soup . smooth () soup . p . contents # [u'A one, a two'] print ( soup . p . prettify ()) # <p> # A one, a two # </p> The smooth() method is new in Beautiful Soup 4.8.0. Output Â¶ Pretty-printing Â¶ The prettify() method will turn a Beautiful Soup parse tree into a\nnicely formatted Unicode string, with a separate line for each\ntag and each string: markup = '<a href=\"http://example.com/\">I linked to <i>example.com</i></a>' soup = BeautifulSoup ( markup ) soup . prettify () # '<html>\\n <head>\\n </head>\\n <body>\\n <a href=\"http://example.com/\">\\n...' print ( soup . prettify ()) # <html> # <head> # </head> # <body> # <a href=\"http://example.com/\"> # I linked to # <i> # example.com # </i> # </a> # </body> # </html> You can call prettify() on the top-level BeautifulSoup object,\nor on any of its Tag objects: print ( soup . a . prettify ()) # <a href=\"http://example.com/\"> # I linked to # <i> # example.com # </i> # </a> Non-pretty printing Â¶ If you just want a string, with no fancy formatting, you can call unicode() or str() on a BeautifulSoup object, or a Tag within it: str ( soup ) # '<html><head></head><body><a href=\"http://example.com/\">I linked to <i>example.com</i></a></body></html>' unicode ( soup . a ) # u'<a href=\"http://example.com/\">I linked to <i>example.com</i></a>' The str() function returns a string encoded in UTF-8. See Encodings for other options. You can also call encode() to get a bytestring, and decode() to get Unicode. Output formatters Â¶ If you give Beautiful Soup a document that contains HTML entities like\nâ&lquot;â, theyâll be converted to Unicode characters: soup = BeautifulSoup ( \"&ldquo;Dammit!&rdquo; he said.\" ) unicode ( soup ) # u'<html><head></head><body>\\u201cDammit!\\u201d he said.</body></html>' If you then convert the document to a string, the Unicode characters\nwill be encoded as UTF-8. You wonât get the HTML entities back: str ( soup ) # '<html><head></head><body>\\xe2\\x80\\x9cDammit!\\xe2\\x80\\x9d he said.</body></html>' By default, the only characters that are escaped upon output are bare\nampersands and angle brackets. These get turned into â&amp;â, â&lt;â,\nand â&gt;â, so that Beautiful Soup doesnât inadvertently generate\ninvalid HTML or XML: soup = BeautifulSoup ( \"<p>The law firm of Dewey, Cheatem, & Howe</p>\" ) soup . p # <p>The law firm of Dewey, Cheatem, &amp; Howe</p> soup = BeautifulSoup ( '<a href=\"http://example.com/?foo=val1&bar=val2\">A link</a>' ) soup . a # <a href=\"http://example.com/?foo=val1&amp;bar=val2\">A link</a> You can change this behavior by providing a value for the formatter argument to prettify() , encode() , or decode() . Beautiful Soup recognizes five possible values for formatter . The default is formatter=\"minimal\" . Strings will only be processed\nenough to ensure that Beautiful Soup generates valid HTML/XML: french = \"<p>Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt;</p>\" soup = BeautifulSoup ( french ) print ( soup . prettify ( formatter = \"minimal\" )) # <html> # <body> # <p> # Il a dit &lt;&lt;SacrÃ© bleu!&gt;&gt; # </p> # </body> # </html> If you pass in formatter=\"html\" , Beautiful Soup will convert\nUnicode characters to HTML entities whenever possible: print ( soup . prettify ( formatter = \"html\" )) # <html> # <body> # <p> # Il a dit &lt;&lt;Sacr&eacute; bleu!&gt;&gt; # </p> # </body> # </html> If you pass in formatter=\"html5\" , itâs the same as formatter=\"html5\" , but Beautiful Soup will\nomit the closing slash in HTML void tags like âbrâ: soup = BeautifulSoup ( \"<br>\" ) print ( soup . encode ( formatter = \"html\" )) # <html><body><br/></body></html> print ( soup . encode ( formatter = \"html5\" )) # <html><body><br></body></html> If you pass in formatter=None , Beautiful Soup will not modify\nstrings at all on output. This is the fastest option, but it may lead\nto Beautiful Soup generating invalid HTML/XML, as in these examples: print ( soup . prettify ( formatter = None )) # <html> # <body> # <p> # Il a dit <<SacrÃ© bleu!>> # </p> # </body> # </html> link_soup = BeautifulSoup ( '<a href=\"http://example.com/?foo=val1&bar=val2\">A link</a>' ) print ( link_soup . a . encode ( formatter = None )) # <a href=\"http://example.com/?foo=val1&bar=val2\">A link</a> If you need more sophisticated control over your output, you can\nuse Beautiful Soupâs Formatter class. Hereâs a formatter that\nconverts strings to uppercase, whether they occur in a text node or in an\nattribute value: from bs4.formatter import HTMLFormatter def uppercase ( str ): return str . upper () formatter = HTMLFormatter ( uppercase ) print ( soup . prettify ( formatter = formatter )) # <html> # <body> # <p> # IL A DIT <<SACRÃ BLEU!>> # </p> # </body> # </html> print ( link_soup . a . prettify ( formatter = formatter )) # <a href=\"HTTP://EXAMPLE.COM/?FOO=VAL1&BAR=VAL2\"> # A LINK # </a> Subclassing HTMLFormatter or XMLFormatter will give you even\nmore control over the output. For example, Beautiful Soup sorts the\nattributes in every tag by default: attr_soup = BeautifulSoup ( b '<p z=\"1\" m=\"2\" a=\"3\"></p>' ) print ( attr_soup . p . encode ()) # <p a=\"3\" m=\"2\" z=\"1\"></p> To turn this off, you can subclass the Formatter.attributes() method, which controls which attributes are output and in what\norder. This implementation also filters out the attribute called âmâ\nwhenever it appears: class UnsortedAttributes ( HTMLFormatter ): def attributes ( self , tag ): for k , v in tag . attrs . items (): if k == 'm' : continue yield k , v print ( attr_soup . p . encode ( formatter = UnsortedAttributes ())) # <p z=\"1\" a=\"3\"></p> One last caveat: if you create a CData object, the text inside\nthat object is always presented exactly as it appears, with no\nformatting . Beautiful Soup will call your entity substitution\nfunction, just in case youâve written a custom function that counts\nall the strings in the document or something, but it will ignore the\nreturn value: from bs4.element import CData soup = BeautifulSoup ( \"<a></a>\" ) soup . a . string = CData ( \"one < three\" ) print ( soup . a . prettify ( formatter = \"xml\" )) # <a> # <![CDATA[one < three]]> # </a> get_text() Â¶ If you only want the text part of a document or tag, you can use the get_text() method. It returns all the text in a document or\nbeneath a tag, as a single Unicode string: markup = '<a href=\"http://example.com/\"> \\n I linked to <i>example.com</i> \\n </a>' soup = BeautifulSoup ( markup ) soup . get_text () u ' \\n I linked to example.com \\n ' soup . i . get_text () u 'example.com' You can specify a string to be used to join the bits of text\ntogether: # soup.get_text(\"|\") u ' \\n I linked to |example.com| \\n ' You can tell Beautiful Soup to strip whitespace from the beginning and\nend of each bit of text: # soup.get_text(\"|\", strip=True) u 'I linked to|example.com' But at that point you might want to use the .stripped_strings generator instead, and process the text yourself: [ text for text in soup . stripped_strings ] # [u'I linked to', u'example.com'] Specifying the parser to use Â¶ If you just need to parse some HTML, you can dump the markup into the BeautifulSoup constructor, and itâll probably be fine. Beautiful\nSoup will pick a parser for you and parse the data. But there are a\nfew additional arguments you can pass in to the constructor to change\nwhich parser is used. The first argument to the BeautifulSoup constructor is a string or\nan open filehandleâthe markup you want parsed. The second argument is how youâd like the markup parsed. If you donât specify anything, youâll get the best HTML parser thatâs\ninstalled. Beautiful Soup ranks lxmlâs parser as being the best, then\nhtml5libâs, then Pythonâs built-in parser. You can override this by\nspecifying one of the following: What type of markup you want to parse. Currently supported are\nâhtmlâ, âxmlâ, and âhtml5â. The name of the parser library you want to use. Currently supported\noptions are âlxmlâ, âhtml5libâ, and âhtml.parserâ (Pythonâs\nbuilt-in HTML parser). The section Installing a parser contrasts the supported parsers. If you donât have an appropriate parser installed, Beautiful Soup will\nignore your request and pick a different parser. Right now, the only\nsupported XML parser is lxml. If you donât have lxml installed, asking\nfor an XML parser wonât give you one, and asking for âlxmlâ wonât work\neither. Differences between parsers Â¶ Beautiful Soup presents the same interface to a number of different\nparsers, but each parser is different. Different parsers will create\ndifferent parse trees from the same document. The biggest differences\nare between the HTML parsers and the XML parsers. Hereâs a short\ndocument, parsed as HTML: BeautifulSoup ( \"<a><b /></a>\" ) # <html><head></head><body><a><b></b></a></body></html> Since an empty <b /> tag is not valid HTML, the parser turns it into a\n<b></b> tag pair. Hereâs the same document parsed as XML (running this requires that you\nhave lxml installed). Note that the empty <b /> tag is left alone, and\nthat the document is given an XML declaration instead of being put\ninto an <html> tag.: BeautifulSoup ( \"<a><b /></a>\" , \"xml\" ) # <?xml version=\"1.0\" encoding=\"utf-8\"?> # <a><b/></a> There are also differences between HTML parsers. If you give Beautiful\nSoup a perfectly-formed HTML document, these differences wonât\nmatter. One parser will be faster than another, but theyâll all give\nyou a data structure that looks exactly like the original HTML\ndocument. But if the document is not perfectly-formed, different parsers will\ngive different results. Hereâs a short, invalid document parsed using\nlxmlâs HTML parser. Note that the dangling </p> tag is simply\nignored: BeautifulSoup ( \"<a></p>\" , \"lxml\" ) # <html><body><a></a></body></html> Hereâs the same document parsed using html5lib: BeautifulSoup ( \"<a></p>\" , \"html5lib\" ) # <html><head></head><body><a><p></p></a></body></html> Instead of ignoring the dangling </p> tag, html5lib pairs it with an\nopening <p> tag. This parser also adds an empty <head> tag to the\ndocument. Hereâs the same document parsed with Pythonâs built-in HTML\nparser: BeautifulSoup ( \"<a></p>\" , \"html.parser\" ) # <a></a> Like html5lib, this parser ignores the closing </p> tag. Unlike\nhtml5lib, this parser makes no attempt to create a well-formed HTML\ndocument by adding a <body> tag. Unlike lxml, it doesnât even bother\nto add an <html> tag. Since the document â<a></p>â is invalid, none of these techniques is\nthe âcorrectâ way to handle it. The html5lib parser uses techniques\nthat are part of the HTML5 standard, so it has the best claim on being\nthe âcorrectâ way, but all three techniques are legitimate. Differences between parsers can affect your script. If youâre planning\non distributing your script to other people, or running it on multiple\nmachines, you should specify a parser in the BeautifulSoup constructor. That will reduce the chances that your users parse a\ndocument differently from the way you parse it. Encodings Â¶ Any HTML or XML document is written in a specific encoding like ASCII\nor UTF-8. But when you load that document into Beautiful Soup, youâll\ndiscover itâs been converted to Unicode: markup = \"<h1>Sacr \\xc3\\xa9 bleu!</h1>\" soup = BeautifulSoup ( markup ) soup . h1 # <h1>SacrÃ© bleu!</h1> soup . h1 . string # u'Sacr\\xe9 bleu!' Itâs not magic. (That sure would be nice.) Beautiful Soup uses a\nsub-library called Unicode, Dammit to detect a documentâs encoding\nand convert it to Unicode. The autodetected encoding is available as\nthe .original_encoding attribute of the BeautifulSoup object: soup . original_encoding 'utf-8' Unicode, Dammit guesses correctly most of the time, but sometimes it\nmakes mistakes. Sometimes it guesses correctly, but only after a\nbyte-by-byte search of the document that takes a very long time. If\nyou happen to know a documentâs encoding ahead of time, you can avoid\nmistakes and delays by passing it to the BeautifulSoup constructor\nas from_encoding . Hereâs a document written in ISO-8859-8. The document is so short that\nUnicode, Dammit canât get a lock on it, and misidentifies it as\nISO-8859-7: markup = b \"<h1> \\xed\\xe5\\xec\\xf9 </h1>\" soup = BeautifulSoup ( markup ) soup . h1 < h1 > Î½ÎµÎ¼Ï </ h1 > soup . original_encoding 'ISO-8859-7' We can fix this by passing in the correct from_encoding : soup = BeautifulSoup ( markup , from_encoding = \"iso-8859-8\" ) soup . h1 < h1 > ××××© </ h1 > soup . original_encoding 'iso8859-8' If you donât know what the correct encoding is, but you know that\nUnicode, Dammit is guessing wrong, you can pass the wrong guesses in\nas exclude_encodings : soup = BeautifulSoup ( markup , exclude_encodings = [ \"ISO-8859-7\" ]) soup . h1 < h1 > ××××© </ h1 > soup . original_encoding 'WINDOWS-1255' Windows-1255 isnât 100% correct, but that encoding is a compatible\nsuperset of ISO-8859-8, so itâs close enough. ( exclude_encodings is a new feature in Beautiful Soup 4.4.0.) In rare cases (usually when a UTF-8 document contains text written in\na completely different encoding), the only way to get Unicode may be\nto replace some characters with the special Unicode character\nâREPLACEMENT CHARACTERâ (U+FFFD, ï¿½). If Unicode, Dammit needs to do\nthis, it will set the .contains_replacement_characters attribute\nto True on the UnicodeDammit or BeautifulSoup object. This\nlets you know that the Unicode representation is not an exact\nrepresentation of the originalâsome data was lost. If a document\ncontains ï¿½, but .contains_replacement_characters is False ,\nyouâll know that the ï¿½ was there originally (as it is in this\nparagraph) and doesnât stand in for missing data. Output encoding Â¶ When you write out a document from Beautiful Soup, you get a UTF-8\ndocument, even if the document wasnât in UTF-8 to begin with. Hereâs a\ndocument written in the Latin-1 encoding: markup = b ''' <html> <head> <meta content=\"text/html; charset=ISO-Latin-1\" http-equiv=\"Content-type\" /> </head> <body> <p>Sacr \\xe9 bleu!</p> </body> </html> ''' soup = BeautifulSoup ( markup ) print ( soup . prettify ()) # <html> # <head> # <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\" /> # </head> # <body> # <p> # SacrÃ© bleu! # </p> # </body> # </html> Note that the <meta> tag has been rewritten to reflect the fact that\nthe document is now in UTF-8. If you donât want UTF-8, you can pass an encoding into prettify() : print ( soup . prettify ( \"latin-1\" )) # <html> # <head> # <meta content=\"text/html; charset=latin-1\" http-equiv=\"Content-type\" /> # ... You can also call encode() on the BeautifulSoup object, or any\nelement in the soup, just as if it were a Python string: soup . p . encode ( \"latin-1\" ) # '<p>Sacr\\xe9 bleu!</p>' soup . p . encode ( \"utf-8\" ) # '<p>Sacr\\xc3\\xa9 bleu!</p>' Any characters that canât be represented in your chosen encoding will\nbe converted into numeric XML entity references. Hereâs a document\nthat includes the Unicode character SNOWMAN: markup = u \"<b> \\N{SNOWMAN} </b>\" snowman_soup = BeautifulSoup ( markup ) tag = snowman_soup . b The SNOWMAN character can be part of a UTF-8 document (it looks like\nâ), but thereâs no representation for that character in ISO-Latin-1 or\nASCII, so itâs converted into â&#9731â for those encodings: print ( tag . encode ( \"utf-8\" )) # <b>â</b> print tag . encode ( \"latin-1\" ) # <b>&#9731;</b> print tag . encode ( \"ascii\" ) # <b>&#9731;</b> Unicode, Dammit Â¶ You can use Unicode, Dammit without using Beautiful Soup. Itâs useful\nwhenever you have data in an unknown encoding and you just want it to\nbecome Unicode: from bs4 import UnicodeDammit dammit = UnicodeDammit ( \"Sacr \\xc3\\xa9 bleu!\" ) print ( dammit . unicode_markup ) # SacrÃ© bleu! dammit . original_encoding # 'utf-8' Unicode, Dammitâs guesses will get a lot more accurate if you install\nthe chardet or cchardet Python libraries. The more data you\ngive Unicode, Dammit, the more accurately it will guess. If you have\nyour own suspicions as to what the encoding might be, you can pass\nthem in as a list: dammit = UnicodeDammit ( \"Sacr \\xe9 bleu!\" , [ \"latin-1\" , \"iso-8859-1\" ]) print ( dammit . unicode_markup ) # SacrÃ© bleu! dammit . original_encoding # 'latin-1' Unicode, Dammit has two special features that Beautiful Soup doesnât\nuse. Smart quotes Â¶ You can use Unicode, Dammit to convert Microsoft smart quotes to HTML or XML\nentities: markup = b \"<p>I just \\x93 love \\x94 Microsoft Word \\x92 s smart quotes</p>\" UnicodeDammit ( markup , [ \"windows-1252\" ], smart_quotes_to = \"html\" ) . unicode_markup # u'<p>I just &ldquo;love&rdquo; Microsoft Word&rsquo;s smart quotes</p>' UnicodeDammit ( markup , [ \"windows-1252\" ], smart_quotes_to = \"xml\" ) . unicode_markup # u'<p>I just &#x201C;love&#x201D; Microsoft Word&#x2019;s smart quotes</p>' You can also convert Microsoft smart quotes to ASCII quotes: UnicodeDammit ( markup , [ \"windows-1252\" ], smart_quotes_to = \"ascii\" ) . unicode_markup # u'<p>I just \"love\" Microsoft Word\\'s smart quotes</p>' Hopefully youâll find this feature useful, but Beautiful Soup doesnât\nuse it. Beautiful Soup prefers the default behavior, which is to\nconvert Microsoft smart quotes to Unicode characters along with\neverything else: UnicodeDammit ( markup , [ \"windows-1252\" ]) . unicode_markup # u'<p>I just \\u201clove\\u201d Microsoft Word\\u2019s smart quotes</p>' Inconsistent encodings Â¶ Sometimes a document is mostly in UTF-8, but contains Windows-1252\ncharacters such as (again) Microsoft smart quotes. This can happen\nwhen a website includes data from multiple sources. You can use UnicodeDammit.detwingle() to turn such a document into pure\nUTF-8. Hereâs a simple example: snowmen = ( u \" \\N{SNOWMAN} \" * 3 ) quote = ( u \" \\N{LEFT DOUBLE QUOTATION MARK} I like snowmen! \\N{RIGHT DOUBLE QUOTATION MARK} \" ) doc = snowmen . encode ( \"utf8\" ) + quote . encode ( \"windows_1252\" ) This document is a mess. The snowmen are in UTF-8 and the quotes are\nin Windows-1252. You can display the snowmen or the quotes, but not\nboth: print ( doc ) # âââï¿½I like snowmen!ï¿½ print ( doc . decode ( \"windows-1252\" )) # Ã¢ËÆÃ¢ËÆÃ¢ËÆâI like snowmen!â Decoding the document as UTF-8 raises a UnicodeDecodeError , and\ndecoding it as Windows-1252 gives you gibberish. Fortunately, UnicodeDammit.detwingle() will convert the string to pure UTF-8,\nallowing you to decode it to Unicode and display the snowmen and quote\nmarks simultaneously: new_doc = UnicodeDammit . detwingle ( doc ) print ( new_doc . decode ( \"utf8\" )) # ââââI like snowmen!â UnicodeDammit.detwingle() only knows how to handle Windows-1252\nembedded in UTF-8 (or vice versa, I suppose), but this is the most\ncommon case. Note that you must know to call UnicodeDammit.detwingle() on your\ndata before passing it into BeautifulSoup or the UnicodeDammit constructor. Beautiful Soup assumes that a document has a single\nencoding, whatever it might be. If you pass it a document that\ncontains both UTF-8 and Windows-1252, itâs likely to think the whole\ndocument is Windows-1252, and the document will come out looking like Ã¢ËÆÃ¢ËÆÃ¢ËÆâI like snowmen!â . UnicodeDammit.detwingle() is new in Beautiful Soup 4.1.0. Line numbers Â¶ The html.parser` and ``html5lib parsers can keep track of where in\nthe original document each Tag was found. You can access this\ninformation as Tag.sourceline (line number) and Tag.sourcepos (position of the start tag within a line): markup = \"<p \\n >Paragraph 1</p> \\n <p>Paragraph 2</p>\" soup = BeautifulSoup ( markup , 'html.parser' ) for tag in soup . find_all ( 'p' ): print ( tag . sourceline , tag . sourcepos , tag . string ) # (1, 0, u'Paragraph 1') # (2, 3, u'Paragraph 2') Note that the two parsers mean slightly different things by sourceline and sourcepos . For html.parser, these numbers\nrepresent the position of the initial less-than sign. For html5lib,\nthese numbers represent the position of the final greater-than sign: soup = BeautifulSoup ( markup , 'html5lib' ) for tag in soup . find_all ( 'p' ): print ( tag . sourceline , tag . sourcepos , tag . string ) # (2, 1, u'Paragraph 1') # (3, 7, u'Paragraph 2') You can shut off this feature by passing store_line_numbers=False` into the ``BeautifulSoup constructor: markup = \"<p \\n >Paragraph 1</p> \\n <p>Paragraph 2</p>\" soup = BeautifulSoup ( markup , 'html.parser' , store_line_numbers = False ) soup . p . sourceline # None This feature is new in 4.8.1, and the parsers based on lxml donât\nsupport it. Comparing objects for equality Â¶ Beautiful Soup says that two NavigableString or Tag objects\nare equal when they represent the same HTML or XML markup. In this\nexample, the two <b> tags are treated as equal, even though they live\nin different parts of the object tree, because they both look like\nâ<b>pizza</b>â: markup = \"<p>I want <b>pizza</b> and more <b>pizza</b>!</p>\" soup = BeautifulSoup ( markup , 'html.parser' ) first_b , second_b = soup . find_all ( 'b' ) print first_b == second_b # True print first_b . previous_element == second_b . previous_element # False If you want to see whether two variables refer to exactly the same\nobject, use is : print first_b is second_b # False Copying Beautiful Soup objects Â¶ You can use copy.copy() to create a copy of any Tag or NavigableString : import copy p_copy = copy . copy ( soup . p ) print p_copy # <p>I want <b>pizza</b> and more <b>pizza</b>!</p> The copy is considered equal to the original, since it represents the\nsame markup as the original, but itâs not the same object: print soup . p == p_copy # True print soup . p is p_copy # False The only real difference is that the copy is completely detached from\nthe original Beautiful Soup object tree, just as if extract() had\nbeen called on it: print p_copy . parent # None This is because two different Tag objects canât occupy the same\nspace at the same time. Parsing only part of a document Â¶ Letâs say you want to use Beautiful Soup look at a documentâs <a>\ntags. Itâs a waste of time and memory to parse the entire document and\nthen go over it again looking for <a> tags. It would be much faster to\nignore everything that wasnât an <a> tag in the first place. The SoupStrainer class allows you to choose which parts of an incoming\ndocument are parsed. You just create a SoupStrainer and pass it in\nto the BeautifulSoup constructor as the parse_only argument. (Note that this feature wonât work if youâre using the html5lib parser .\nIf you use html5lib, the whole document will be parsed, no\nmatter what. This is because html5lib constantly rearranges the parse\ntree as it works, and if some part of the document didnât actually\nmake it into the parse tree, itâll crash. To avoid confusion, in the\nexamples below Iâll be forcing Beautiful Soup to use Pythonâs\nbuilt-in parser.) SoupStrainer Â¶ The SoupStrainer class takes the same arguments as a typical\nmethod from Searching the tree : name , attrs , string , and **kwargs . Here are\nthree SoupStrainer objects: from bs4 import SoupStrainer only_a_tags = SoupStrainer ( \"a\" ) only_tags_with_id_link2 = SoupStrainer ( id = \"link2\" ) def is_short_string ( string ): return len ( string ) < 10 only_short_strings = SoupStrainer ( string = is_short_string ) Iâm going to bring back the âthree sistersâ document one more time,\nand weâll see what the document looks like when itâs parsed with these\nthree SoupStrainer objects: html_doc = \"\"\" <html><head><title>The Dormouse's story</title></head> <body> <p class=\"title\"><b>The Dormouse's story</b></p> <p class=\"story\">Once upon a time there were three little sisters; and their names were <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>, <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and <a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>; and they lived at the bottom of a well.</p> <p class=\"story\">...</p> \"\"\" print ( BeautifulSoup ( html_doc , \"html.parser\" , parse_only = only_a_tags ) . prettify ()) # <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"> # Elsie # </a> # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> # Lacie # </a> # <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"> # Tillie # </a> print ( BeautifulSoup ( html_doc , \"html.parser\" , parse_only = only_tags_with_id_link2 ) . prettify ()) # <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"> # Lacie # </a> print ( BeautifulSoup ( html_doc , \"html.parser\" , parse_only = only_short_strings ) . prettify ()) # Elsie # , # Lacie # and # Tillie # ... # You can also pass a SoupStrainer into any of the methods covered\nin Searching the tree . This probably isnât terribly useful, but I\nthought Iâd mention it: soup = BeautifulSoup ( html_doc ) soup . find_all ( only_short_strings ) # [u'\\n\\n', u'\\n\\n', u'Elsie', u',\\n', u'Lacie', u' and\\n', u'Tillie', # u'\\n\\n', u'...', u'\\n'] Troubleshooting Â¶ diagnose() Â¶ If youâre having trouble understanding what Beautiful Soup does to a\ndocument, pass the document into the diagnose() function. (New in\nBeautiful Soup 4.2.0.) Beautiful Soup will print out a report showing\nyou how different parsers handle the document, and tell you if youâre\nmissing a parser that Beautiful Soup could be using: from bs4.diagnose import diagnose with open ( \"bad.html\" ) as fp : data = fp . read () diagnose ( data ) # Diagnostic running on Beautiful Soup 4.2.0 # Python version 2.7.3 (default, Aug 1 2012, 05:16:07) # I noticed that html5lib is not installed. Installing it may help. # Found lxml version 2.3.2.0 # # Trying to parse your data with html.parser # Here's what html.parser did with the document: # ... Just looking at the output of diagnose() may show you how to solve the\nproblem. Even if not, you can paste the output of diagnose() when\nasking for help. Errors when parsing a document Â¶ There are two different kinds of parse errors. There are crashes,\nwhere you feed a document to Beautiful Soup and it raises an\nexception, usually an HTMLParser.HTMLParseError . And there is\nunexpected behavior, where a Beautiful Soup parse tree looks a lot\ndifferent than the document used to create it. Almost none of these problems turn out to be problems with Beautiful\nSoup. This is not because Beautiful Soup is an amazingly well-written\npiece of software. Itâs because Beautiful Soup doesnât include any\nparsing code. Instead, it relies on external parsers. If one parser\nisnât working on a certain document, the best solution is to try a\ndifferent parser. See Installing a parser for details and a parser\ncomparison. The most common parse errors are HTMLParser.HTMLParseError: malformed start tag and HTMLParser.HTMLParseError: bad end tag . These are both generated by Pythonâs built-in HTML parser\nlibrary, and the solution is to install lxml or\nhtml5lib. The most common type of unexpected behavior is that you canât find a\ntag that you know is in the document. You saw it going in, but find_all() returns [] or find() returns None . This is\nanother common problem with Pythonâs built-in HTML parser, which\nsometimes skips tags it doesnât understand. Again, the solution is to install lxml or html5lib. Version mismatch problems Â¶ SyntaxError: Invalid syntax (on the line ROOT_TAG_NAME = u'[document]' ): Caused by running the Python 2 version of\nBeautiful Soup under Python 3, without converting the code. ImportError: No module named HTMLParser - Caused by running the\nPython 2 version of Beautiful Soup under Python 3. ImportError: No module named html.parser - Caused by running the\nPython 3 version of Beautiful Soup under Python 2. ImportError: No module named BeautifulSoup - Caused by running\nBeautiful Soup 3 code on a system that doesnât have BS3\ninstalled. Or, by writing Beautiful Soup 4 code without knowing that\nthe package name has changed to bs4 . ImportError: No module named bs4 - Caused by running Beautiful\nSoup 4 code on a system that doesnât have BS4 installed. Parsing XML Â¶ By default, Beautiful Soup parses documents as HTML. To parse a\ndocument as XML, pass in âxmlâ as the second argument to the BeautifulSoup constructor: soup = BeautifulSoup ( markup , \"xml\" ) Youâll need to have lxml installed . Other parser problems Â¶ If your script works on one computer but not another, or in one\nvirtual environment but not another, or outside the virtual\nenvironment but not inside, itâs probably because the two\nenvironments have different parser libraries available. For example,\nyou may have developed the script on a computer that has lxml\ninstalled, and then tried to run it on a computer that only has\nhtml5lib installed. See Differences between parsers for why this\nmatters, and fix the problem by mentioning a specific parser library\nin the BeautifulSoup constructor. Because HTML tags and attributes are case-insensitive , all three HTML\nparsers convert tag and attribute names to lowercase. That is, the\nmarkup <TAG></TAG> is converted to <tag></tag>. If you want to\npreserve mixed-case or uppercase tags and attributes, youâll need to parse the document as XML. Miscellaneous Â¶ UnicodeEncodeError: 'charmap' codec can't encode character u'\\xfoo' in position bar (or just about any other UnicodeEncodeError ) - This is not a problem with Beautiful Soup.\nThis problem shows up in two main situations. First, when you try to\nprint a Unicode character that your console doesnât know how to\ndisplay. (See this page on the Python wiki for help.) Second, when\nyouâre writing to a file and you pass in a Unicode character thatâs\nnot supported by your default encoding. In this case, the simplest\nsolution is to explicitly encode the Unicode string into UTF-8 with u.encode(\"utf8\") . KeyError: [attr] - Caused by accessing tag['attr'] when the\ntag in question doesnât define the attr attribute. The most\ncommon errors are KeyError: 'href' and KeyError: 'class' . Use tag.get('attr') if youâre not sure attr is\ndefined, just as you would with a Python dictionary. AttributeError: 'ResultSet' object has no attribute 'foo' - This\nusually happens because you expected find_all() to return a\nsingle tag or string. But find_all() returns a _list_ of tags\nand stringsâa ResultSet object. You need to iterate over the\nlist and look at the .foo of each one. Or, if you really only\nwant one result, you need to use find() instead of find_all() . AttributeError: 'NoneType' object has no attribute 'foo' - This\nusually happens because you called find() and then tried to\naccess the .foo` attribute of the result. But in your case, find() didnât find anything, so it returned None , instead of\nreturning a tag or a string. You need to figure out why your find() call isnât returning anything. Improving Performance Â¶ Beautiful Soup will never be as fast as the parsers it sits on top\nof. If response time is critical, if youâre paying for computer time\nby the hour, or if thereâs any other reason why computer time is more\nvaluable than programmer time, you should forget about Beautiful Soup\nand work directly atop lxml . That said, there are things you can do to speed up Beautiful Soup. If\nyouâre not using lxml as the underlying parser, my advice is to start . Beautiful Soup parses documents\nsignificantly faster using lxml than using html.parser or html5lib. You can speed up encoding detection significantly by installing the cchardet library. Parsing only part of a document wonât save you much time parsing\nthe document, but it can save a lot of memory, and itâll make searching the document much faster. Translating this documentation Â¶ New translations of the Beautiful Soup documentation are greatly\nappreciated. Translations should be licensed under the MIT license,\njust like Beautiful Soup and its English documentation are. There are two ways of getting your translation into the main code base\nand onto the Beautiful Soup website: Create a branch of the Beautiful Soup repository, add your\ntranslation, and propose a merge with the main branch, the same\nas you would do with a proposed change to the source code. Send a message to the Beautiful Soup discussion group with a link to\nyour translation, or attach your translation to the message. Use the Chinese or Brazilian Portuguese translations as your model. In\nparticular, please translate the source file doc/source/index.rst ,\nrather than the HTML version of the documentation. This makes it\npossible to publish the documentation in a variety of formats, not\njust HTML. Beautiful Soup 3 Â¶ Beautiful Soup 3 is the previous release series, and is no longer\nbeing actively developed. Itâs currently packaged with all major Linux\ndistributions: $ apt-get install python-beautifulsoup Itâs also published through PyPi as BeautifulSoup .: $ easy_install BeautifulSoup $ pip install BeautifulSoup You can also download a tarball of Beautiful Soup 3.2.0 . If you ran easy_install beautifulsoup or easy_install BeautifulSoup , but your code doesnât work, you installed Beautiful\nSoup 3 by mistake. You need to run easy_install beautifulsoup4 . The documentation for Beautiful Soup 3 is archived online . Porting code to BS4 Â¶ Most code written against Beautiful Soup 3 will work against Beautiful\nSoup 4 with one simple change. All you should have to do is change the\npackage name from BeautifulSoup to bs4 . So this: from BeautifulSoup import BeautifulSoup becomes this: from bs4 import BeautifulSoup If you get the ImportError âNo module named BeautifulSoupâ, your\nproblem is that youâre trying to run Beautiful Soup 3 code, but you\nonly have Beautiful Soup 4 installed. If you get the ImportError âNo module named bs4â, your problem\nis that youâre trying to run Beautiful Soup 4 code, but you only\nhave Beautiful Soup 3 installed. Although BS4 is mostly backwards-compatible with BS3, most of its\nmethods have been deprecated and given new names for PEP 8 compliance . There are numerous other\nrenames and changes, and a few of them break backwards compatibility. Hereâs what youâll need to know to convert your BS3 code and habits to BS4: You need a parser Â¶ Beautiful Soup 3 used Pythonâs SGMLParser , a module that was\ndeprecated and removed in Python 3.0. Beautiful Soup 4 uses html.parser by default, but you can plug in lxml or html5lib and\nuse that instead. See Installing a parser for a comparison. Since html.parser is not the same parser as SGMLParser , you\nmay find that Beautiful Soup 4 gives you a different parse tree than\nBeautiful Soup 3 for the same markup. If you swap out html.parser for lxml or html5lib, you may find that the parse tree changes yet\nagain. If this happens, youâll need to update your scraping code to\ndeal with the new tree. Method names Â¶ renderContents -> encode_contents replaceWith -> replace_with replaceWithChildren -> unwrap findAll -> find_all findAllNext -> find_all_next findAllPrevious -> find_all_previous findNext -> find_next findNextSibling -> find_next_sibling findNextSiblings -> find_next_siblings findParent -> find_parent findParents -> find_parents findPrevious -> find_previous findPreviousSibling -> find_previous_sibling findPreviousSiblings -> find_previous_siblings getText -> get_text nextSibling -> next_sibling previousSibling -> previous_sibling Some arguments to the Beautiful Soup constructor were renamed for the\nsame reasons: BeautifulSoup(parseOnlyThese=...) -> BeautifulSoup(parse_only=...) BeautifulSoup(fromEncoding=...) -> BeautifulSoup(from_encoding=...) I renamed one method for compatibility with Python 3: Tag.has_key() -> Tag.has_attr() I renamed one attribute to use more accurate terminology: Tag.isSelfClosing -> Tag.is_empty_element I renamed three attributes to avoid using words that have special\nmeaning to Python. Unlike the others, these changes are not backwards\ncompatible. If you used these attributes in BS3, your code will break\non BS4 until you change them. UnicodeDammit.unicode -> UnicodeDammit.unicode_markup Tag.next -> Tag.next_element Tag.previous -> Tag.previous_element Generators Â¶ I gave the generators PEP 8-compliant names, and transformed them into\nproperties: childGenerator() -> children nextGenerator() -> next_elements nextSiblingGenerator() -> next_siblings previousGenerator() -> previous_elements previousSiblingGenerator() -> previous_siblings recursiveChildGenerator() -> descendants parentGenerator() -> parents So instead of this: for parent in tag . parentGenerator (): ... You can write this: for parent in tag . parents : ... (But the old code will still work.) Some of the generators used to yield None after they were done, and\nthen stop. That was a bug. Now the generators just stop. There are two new generators, .strings and\n.stripped_strings . .strings yields\nNavigableString objects, and .stripped_strings yields Python\nstrings that have had whitespace stripped. XML Â¶ There is no longer a BeautifulStoneSoup class for parsing XML. To\nparse XML you pass in âxmlâ as the second argument to the BeautifulSoup constructor. For the same reason, the BeautifulSoup constructor no longer recognizes the isHTML argument. Beautiful Soupâs handling of empty-element XML tags has been\nimproved. Previously when you parsed XML you had to explicitly say\nwhich tags were considered empty-element tags. The selfClosingTags argument to the constructor is no longer recognized. Instead,\nBeautiful Soup considers any empty tag to be an empty-element tag. If\nyou add a child to an empty-element tag, it stops being an\nempty-element tag. Entities Â¶ An incoming HTML or XML entity is always converted into the\ncorresponding Unicode character. Beautiful Soup 3 had a number of\noverlapping ways of dealing with entities, which have been\nremoved. The BeautifulSoup constructor no longer recognizes the smartQuotesTo or convertEntities arguments. ( Unicode,\nDammit still has smart_quotes_to , but its default is now to turn\nsmart quotes into Unicode.) The constants HTML_ENTITIES , XML_ENTITIES , and XHTML_ENTITIES have been removed, since they\nconfigure a feature (transforming some but not all entities into\nUnicode characters) that no longer exists. If you want to turn Unicode characters back into HTML entities on\noutput, rather than turning them into UTF-8 characters, you need to\nuse an output formatter . Miscellaneous Â¶ Tag.string now operates recursively. If tag A\ncontains a single tag B and nothing else, then A.string is the same as\nB.string. (Previously, it was None.) Multi-valued attributes like class have lists of strings as\ntheir values, not strings. This may affect the way you search by CSS\nclass. If you pass one of the find* methods both string and a tag-specific argument like name , Beautiful Soup will\nsearch for tags that match your tag-specific criteria and whose Tag.string matches your value for string . It will not find the strings themselves. Previously,\nBeautiful Soup ignored the tag-specific arguments and looked for\nstrings. The BeautifulSoup constructor no longer recognizes the markupMassage argument. Itâs now the parserâs responsibility to\nhandle markup correctly. The rarely-used alternate parser classes like ICantBelieveItsBeautifulSoup and BeautifulSOAP have been\nremoved. Itâs now the parserâs decision how to handle ambiguous\nmarkup. The prettify() method now returns a Unicode string, not a bytestring.", "meta": {"tags": ["blog", "html", "url"], "source_id": "example_blog_post", "source_type": "url", "url": "https://beautiful-soup-4.readthedocs.io/en/latest/", "format": null, "loader_options_used": {}}}
