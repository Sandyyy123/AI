#!/usr/bin/env python3
"""
Phase B (Unified): Embed chunks JSONL -> Chroma (local or remote)

This script reads JSONL files generated by Phase A (containing child chunks and parent documents),
generates embeddings for the child chunks using a specified embedding model, and
stores both child chunks (with embeddings) and parent documents (without embeddings, but with original text)
into separate collections within a ChromaDB instance. The ChromaDB instance's persistence path
is dynamically named based on chunking strategy, document type, embedder, and embedding model.

Command line arguments allow overriding configuration from config.yaml and specifying dynamic naming suffixes.

Embedders supported:
  openai      : OpenAI embeddings via openai SDK
  xai         : xAI (Grok) via OpenAI-compatible endpoint (openai SDK + base_url)
  gemini      : Google Gemini embeddings via REST (batchEmbedContents)
  local_hf    : Hugging Face/SentenceTransformers local models
  ollama      : Ollama local embeddings via REST (/api/embeddings)
  litellm     : LiteLLM embeddings wrapper (many providers)
  voyage      : Voyage embeddings (common choice when you want Claude for generation)

Vector store modes:
  chroma_local: chromadb.PersistentClient(path=...)
  chroma_http : chromadb.HttpClient(host=..., port=...)

Input JSONL format (one chunk per line):
  {"doc_id": "...", "chunk_id": "...", "text": "...", "meta": {...}}
  

Output:
  Chroma collection with explicit embeddings stored (embeddings=... in upsert)
  (Implicitly, a dictionary of parent documents will be available for retrieval in the next phase.)
  """

# ---- SQLITE PATCH (must be first) ----
import sys
import pysqlite3

from scripts.phase_a_build_chunks import get_config_hash

from scripts.phase_a_build_chunks import get_config_hash
sys.modules["sqlite3"] = pysqlite3
# --------------------------------------

# ---- LOAD .env EARLY ----
from dotenv import load_dotenv
load_dotenv()  # loads .env from project root (current working directory)
# -------------------------

import argparse
import json
import os
import re
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple


import yaml
import requests
import chromadb

# Use the vector_stores component
try:
    from scripts.rag.components.vector_stores import VECTOR_STORE_REGISTRY, BaseVectorStore
except ImportError as e:
    logger.critical(f"FATAL: Could not import VECTOR_STORE_REGISTRY from scripts.rag.components.vector_stores: {e}")
    sys.exit(1)

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Internal RAG system imports ---
# Assuming your Chunk dataclass is here for consistent deserialization
try:
    from scripts.rag.components.chunkers import Chunk
    from config import Settings, load_settings
except ImportError as e:
    logger.critical(f"FATAL: Missing internal RAG imports. Ensure project structure "
                    f"and dependencies are correct: {e}")
    sys.exit(1)


# -----------------------------
# Helpers
# -----------------------------
def load_jsonl(path: Path) -> List[Dict[str, Any]]:
    """Loads a JSONL file into a list of dictionaries."""
    rows: List[Dict[str, Any]] = []
    if not path.exists():
        logger.warning(f"File not found: {path}. Returning empty list.")
        return rows
        
    with path.open("r", encoding="utf-8") as f:
        for i, line in enumerate(f, start=1):
            line = line.strip()
            if not line:
                continue
            try:
                rows.append(json.loads(line))
            except json.JSONDecodeError as e:
                logger.error(f"[JSONL ERROR] {path}:{i}: {e}. Skipping line.")
    return rows


def batched(seq: List[Any], batch_size: int) -> List[List[Any]]:
    """Yields successive n-sized chunks from list."""
    return [seq[i : i + batch_size] for i in range(0, len(seq), batch_size)]


def model_slug(s: str) -> str:
    """Converts a string to a URL-friendly slug."""
    s = s.strip().lower()
    s = re.sub(r"[^a-z0-9]+", "_", s)
    return s.strip("_")[:80]


def safe_meta(meta: Dict[str, Any]) -> Dict[str, Any]:
    """
    Chroma metadata values must be: str/int/float/bool/None (no lists/dicts).
    This function coerces unsupported types to JSON strings.
    """
    out: Dict[str, Any] = {}
    for k, v in meta.items():
        if v is None or isinstance(v, (str, int, float, bool)):
            out[k] = v
        elif isinstance(v, (list, tuple, set, dict)):
            try:
                out[k] = json.dumps(v, ensure_ascii=False)
            except TypeError: # Fallback to string if JSON serialization fails
                logger.warning(f"Could not JSON-serialize metadata key '{k}' for Chroma. Converting to string.")
                out[k] = str(v)
        else:
            logger.warning(f"Unsupported metadata type for key '{k}' ({type(v)}). Converting to string.")
            out[k] = str(v)
    return out



# -----------------------------
# Embedders
# -----------------------------
def embed_openai(texts: List[str], model: str) -> List[List[float]]:
    from openai import OpenAI

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise SystemExit("Missing OPENAI_API_KEY")

    client = OpenAI(api_key=api_key)
    resp = client.embeddings.create(model=model, input=texts)
    return [d.embedding for d in resp.data]


def embed_xai(texts: List[str], model: str, base_url: str) -> List[List[float]]:
    """
    xAI uses an OpenAI-compatible API surface. We use OpenAI SDK with base_url.
    """
    from openai import OpenAI

    api_key = os.getenv("XAI_API_KEY")
    if not api_key:
        raise SystemExit("Missing XAI_API_KEY")

    client = OpenAI(api_key=api_key, base_url=base_url)
    resp = client.embeddings.create(model=model, input=texts)
    return [d.embedding for d in resp.data]


def embed_gemini_batch(texts: List[str], model: str, task_type: Optional[str]) -> List[List[float]]:
    """
    Gemini embeddings via REST:
      POST https://generativelanguage.googleapis.com/v1beta/models/{model}:batchEmbedContents
      header: x-goog-api-key: GEMINI_API_KEY
    """
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise SystemExit("Missing GEMINI_API_KEY")

    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:batchEmbedContents"
    headers = {"x-goog-api-key": api_key, "Content-Type": "application/json"}

    reqs = []
    for t in texts:
        r: Dict[str, Any] = {"content": {"parts": [{"text": t}]}}
        if task_type:
            r["taskType"] = task_type  # e.g. RETRIEVAL_DOCUMENT
        reqs.append(r)

    payload = {"requests": reqs}
    resp = requests.post(url, headers=headers, json=payload, timeout=60)
    if resp.status_code != 200:
        raise SystemExit(f"[GEMINI ERROR] {resp.status_code}: {resp.text[:800]}")

    data = resp.json()
    embeddings = data.get("embeddings")
    if not embeddings:
        raise SystemExit(f"[GEMINI ERROR] Unexpected response keys: {list(data.keys())}")

    out: List[List[float]] = []
    for e in embeddings:
        vals = e.get("values")
        if not vals:
            raise SystemExit("[GEMINI ERROR] Missing embedding values")
        out.append(vals)
    return out


def embed_local_hf(texts: List[str], model: str, device: str, normalize: bool) -> List[List[float]]:
    from sentence_transformers import SentenceTransformer

    m = SentenceTransformer(model, device=device)
    vecs = m.encode(
        texts,
        show_progress_bar=False,
        convert_to_numpy=True,
        normalize_embeddings=normalize,
    )
    return vecs.tolist()


def embed_ollama(texts: List[str], model: str, ollama_url: str) -> List[List[float]]:
    """
    Ollama embeddings endpoint:
      POST {ollama_url}/api/embeddings
      {"model":"nomic-embed-text", "prompt":"..."}
    """
    base = ollama_url.rstrip("/")
    endpoint = f"{base}/api/embeddings"

    out: List[List[float]] = []
    for t in texts:
        r = requests.post(endpoint, json={"model": model, "prompt": t}, timeout=60)
        if r.status_code != 200:
            raise SystemExit(f"[OLLAMA ERROR] {r.status_code}: {r.text[:800]}")
        data = r.json()
        emb = data.get("embedding")
        if not emb:
            raise SystemExit("[OLLAMA ERROR] Missing 'embedding' in response")
        out.append(emb)
    return out


def embed_litellm(texts: List[str], model: str) -> List[List[float]]:
    """
    LiteLLM embeddings wrapper. Model strings vary, e.g.:
      openai/text-embedding-3-small
      ollama/nomic-embed-text
      azure/<deployment>
      openai-compatible endpoints if configured in LiteLLM env
    """
    try:
        from litellm import embedding
    except Exception as e:
        raise SystemExit(f"LiteLLM not installed/importable: {type(e).__name__}: {e}")

    # LiteLLM returns: {"data":[{"embedding":[...]}...]}
    resp = embedding(model=model, input=texts)
    data = resp.get("data", [])
    if not data:
        raise SystemExit("[LITELLM ERROR] No data returned from embeddings call")
    return [row["embedding"] for row in data]


def embed_voyage(texts: List[str], model: str, input_type: str) -> List[List[float]]:
    """
    Voyage embeddings (useful when you want Claude for generation).
    Voyage client reads VOYAGE_API_KEY env var.
    """
    try:
        import voyageai
    except Exception as e:
        raise SystemExit(f"voyageai not installed/importable: {type(e).__name__}: {e}")

    vo = voyageai.Client()
    res = vo.embed(texts, model=model, input_type=input_type)
    return res.embeddings


# Your original dispatcher function, now using Settings and rag_config
def get_embeddings(
    embedder: str,
    texts: List[str],
    model: str,
    settings: Settings, # Pass the settings object
    rag_config: Dict[str, Any], # Pass the rag config (for e.g. local_hf device)
) -> List[List[float]]:
    """
    Dispatches to the appropriate embedding function based on the chosen embedder.
    Reads API keys/URLs from the Settings object.
    """
    if embedder == "openai":
        if not settings.OPENAI_API_KEY:
            raise SystemExit("Missing OPENAI_API_KEY environment variable.")
        return embed_openai(texts, model=model, api_key=settings.OPENAI_API_KEY)

    if embedder == "openrouter": # OpenRouter also uses OpenAI compatible API
        if not settings.OPENROUTER_API_KEY:
            raise SystemExit("Missing OPENROUTER_API_KEY environment variable.")
        if not settings.OPENROUTER_BASE_URL: # Assuming base URL is also needed for OpenRouter
            raise SystemExit("Missing OPENROUTER_BASE_URL environment variable.")
        # OpenRouter uses OpenAI SDK, so pass base_url to embed_xai which is designed for that
        return embed_xai(texts, model=model, base_url=settings.OPENROUTER_BASE_URL, api_key=settings.OPENROUTER_API_KEY)

    if embedder == "xai":
        if not settings.XAI_API_KEY: # Make sure XAI_API_KEY is defined in your .env
            raise SystemExit("Missing XAI_API_KEY environment variable.")
        base_url = os.getenv("XAI_BASE_URL", rag_config.get("xai_base_url", "https://api.x.ai/v1")) # Can be in .env or rag_config
        return embed_xai(texts, model=model, base_url=base_url, api_key=settings.XAI_API_KEY)

    if embedder == "gemini":
        if not settings.GOOGLE_API_KEY:
            raise SystemExit("Missing GOOGLE_API_KEY environment variable.")
        gemini_task_type = rag_config.get("gemini_embed_task_type", None) # Can be configured in rag_config
        return embed_gemini_batch(texts, model=model, task_type=gemini_task_type, api_key=settings.GOOGLE_API_KEY)

    if embedder == "local_hf":
        hf_device = rag_config.get("local_hf_embed_device", "cpu")
        hf_normalize = rag_config.get("local_hf_embed_normalize", False)
        return embed_local_hf(texts, model=model, device=hf_device, normalize=hf_normalize)

    if embedder == "ollama":
        if not settings.OLLAMA_BASE_URL: # Assuming OLLAMA_BASE_URL is in .env
            raise SystemExit("Missing OLLAMA_BASE_URL environment variable.")
        return embed_ollama(texts, model=model, ollama_url=settings.OLLAMA_BASE_URL)

    if embedder == "litellm":
        # For LiteLLM, model and API keys are often managed by LiteLLM's own env vars/config
        # We pass it as is, assuming LiteLLM is configured globally
        return embed_litellm(texts, model=model)

    if embedder == "voyage":
        if not settings.VOYAGE_API_KEY: # Voyage API Key assumed in .env
            raise SystemExit("Missing VOYAGE_API_KEY environment variable.")
        voyage_input_type = rag_config.get("voyage_embed_input_type", "document")
        return embed_voyage(texts, model=model, input_type=voyage_input_type)

    raise SystemExit(f"Unknown embedder: {embedder}")

# -----------------------------
# Chroma client factory (YOUR PROVIDED FUNCTION - RESTORED)
# -----------------------------
def get_chroma_collection(
    mode: str,
    collection_name: str,
    persist_directory: Optional[Path] = None,
    host: Optional[str] = None,
    port: Optional[int] = None,
    metadata: Optional[Dict[str, Any]] = None, # Metadata only needed for creation
) -> chromadb.Collection:
    # This is your existing factory function, slightly modified.
    # It must be copied or imported from phase_b_embed.py or a shared utility.
    client: chromadb.ClientAPI
    if mode == "local":
        if not persist_directory or not persist_directory.exists():
            raise SystemExit(f"Chroma mode 'local' requires an existing 'persist_directory': {persist_directory}.")
        client = chromadb.PersistentClient(path=str(persist_directory))
    elif mode == "http":
        if not host or not port:
            raise SystemExit("Chroma mode 'http' requires 'host' and 'port'.")
        client = chromadb.HttpClient(host=host, port=port)
    else:
        raise SystemExit(f"Unknown chroma_mode: {mode}. Supported modes: 'local', 'http'.")

    # On a retrieval side, we just get the collection, don't create/overwrite.
    try:
        collection = client.get_collection(name=collection_name)
    except Exception as e:
        raise SystemExit(f"Failed to get Chroma collection '{collection_name}': {e}. "
                         f"Ensure it exists at {persist_directory or host}:{port}")
    return collection


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Phase B: Embed preprocessed chunks and populate ChromaDB."
    )
    # --- Input paths for chunks ---
    parser.add_argument(
        "--chunks-path",
        type=Path,
        default=None, # Will be dynamically built if not provided
        help="Path to the JSONL file containing preprocessed child chunks (from Phase A)."
    )
    parser.add_argument(
        "--parents-path",
        type=Path,
        default=None, # Will be dynamically built if not provided
        help="Path to the JSONL file containing parent documents (from Phase A)."
    )
    # --- Vector Store settings ---
    parser.add_argument(
        "--vector-store-base-path",
        type=Path,
        default="vector_stores", # Default to a more generic folder name
        help="Base path for local vector store persistence (e.g., 'vector_stores').",
    )
    parser.add_argument(
        "--chroma-mode",
        choices=["local", "http"],
        default="local",
        help="ChromaDB client mode (local or http)."
    )
    parser.add_argument(
        "--chroma-host",
        type=str,
        default=None,
        help="HTTP only: Chroma host"
    )
    parser.add_argument(
        "--chroma-port",
        type=int,
        default=None,
        help="HTTP only: Chroma port"
    )
    parser.add_argument(
        "--child-collection-name",
        type=str,
        default="rag_child_chunks",
        help="Vector store collection name for child chunks (embeddable)."
    )
    parser.add_argument(
        "--parent-collection-name",
        type=str,
        default="rag_parent_documents",
        help="Vector store collection name for parent documents (storage only)."
    )
    parser.add_argument(
        "--config-path",
        type=Path,
        default="config.yaml",
        help="Path to the main configuration YAML file."
    )
    # --- Embedder settings (from CLI to potentially override config.yaml) ---
    parser.add_argument(
        "--embedder",
        type=str,
        default=None, # Default to use config.yaml or 'openrouter'
        choices=["openai", "openrouter", "gemini", "xai", "local_hf", "ollama", "litellm", "voyage"],
        help="Embedding provider (e.g., openai, openrouter, gemini, local_hf)."
    )
    parser.add_argument(
        "--model",
        type=str,
        default=None, # Default to use config.yaml or 'text-embedding-3-small'
        help="Embedding model name for the chosen provider."
    )
    # --- Embedder options ---
    parser.add_argument(
        "--batch-size",
        type=int,
        default=100,
        help="Number of chunks to embed in a single batch."
    )
    parser.add_argument(
        "--sleep",
        type=float,
        default=0.0,
        help="Pause between batches (rate limits) for API calls."
    )
    # --- Specific embedder flags (these align with your get_embeddings arguments) ---
    parser.add_argument("--device", default="cpu", help="local_hf: cpu|cuda")
    parser.add_argument("--normalize", action="store_true", help="local_hf: normalize embeddings.")
    parser.add_argument("--gemini-task-type", default=None, help="gemini: e.g. RETRIEVAL_DOCUMENT.")
    parser.add_argument("--ollama-url", default="http://localhost:11434", help="ollama base URL.")
    parser.add_argument("--voyage-input-type", default="document", help="voyage: document|query.")
    # --- Other options ---
    parser.add_argument(
        "--id-prefix",
        type=str,
        default=None,
        help="Prefix to apply to all Chroma IDs (both child and parent) to avoid collisions."
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Overwrite existing ChromaDB collections if they exist."
    )
    # --- Arguments for dynamic path naming (aligned with Phase A's output) ---
    parser.add_argument(
        "--output-suffix-chunking-strategy",
        type=str,
        default="default",
        help="Chunking strategy name used by Phase A to name output JSONL files (e.g., 'recursive_character'). Also used for ChromaDB path naming."
    )
    parser.add_argument(
        "--output-suffix-doc-type",
        type=str,
        default="docs",
        help="Document type name used by Phase A to name output JSONL files (e.g., 'reports', 'manuals'). Also used for ChromaDB path naming."
    )
    # --- Vector Store Provider Selection ---
    parser.add_argument(
        "--vector-store-provider",
        type=str,
        default=None, # Will default to config.yaml if not provided
        choices=list(VECTOR_STORE_REGISTRY.keys()),
        help="Vector store provider to use (e.g., 'chroma', 'faiss')."
    )
    # --- FAISS Specific Option ---
    parser.add_argument(
        "--faiss-dimension",
        type=int,
        default=None, # Will default to config.yaml if not provided
        help="Dimension of embeddings for FAISS index (required if using FAISS, e.g., 1536)."
    )

    args = parser.parse_args()
    
    # --- Setup ---
    project_root = Path(__file__).resolve().parents[1]
    
    settings = load_settings()
    config_yaml = {}
    if args.config_path.exists():
        with args.config_path.open("r", encoding="utf-8") as f:
            config_yaml = yaml.safe_load(f)
    rag_config = config_yaml.get("rag", {})
    llm_config = config_yaml.get("llm", {})

    #  NEW: Calculate the overall_rag_pipeline_config_hash ---
    overall_rag_pipeline_config_hash = get_config_hash(rag_config)
    logger.info(f"Overall RAG pipeline config hash for this run: {overall_rag_pipeline_config_hash}")

    # Determine embedder_provider and embedding_model, prioritizing CLI args then config.yaml defaults
    embedder_provider = rag_config.get("embedding_provider", llm_config.get("provider", "openrouter"))
    embedding_model = rag_config.get("embedding_model", "text-embedding-3-small")
    
    # Determine vector store provider from CLI --vector-store-provider or config.yaml
    vector_store_global_config = full_config.get("vector_store", {}) # Get the new vector_store section from config.yaml
    selected_vector_store_provider = args.vector_store_provider or vector_store_global_config.get("provider", "chroma")
    
    
    
    logger.info(f"Using embedder provider: {embedder_provider}")
    logger.info(f"Using embedding model: {embedding_model}")
    logger.info(f"Selected Vector Store provider: {selected_vector_store_provider}")
    
    if selected_vector_store_provider not in VECTOR_STORE_REGISTRY:
        logger.critical(f"Configured vector_store provider '{selected_vector_store_provider}' is not registered. Aborting.")
        sys.exit(1)
    
    if not Chunk: 
        logger.critical("Chunk dataclass not available. Check scripts/rag/components/chunkers.py import.")
        sys.exit(1)
    
    # --- RECONSTRUCT DYNAMIC CHROMADB PATH ---
    # --- Dynamic default for input JSONL paths (aligned with Phase A outputs) ---
    dynamic_jsonl_suffix_parts = []
    dynamic_jsonl_suffix_parts.append(model_slug(args.output_suffix_chunking_strategy))
    if args.output_suffix_doc_type:
        dynamic_jsonl_suffix_parts.append(model_slug(args.output_suffix_doc_type))
    dynamic_jsonl_suffix = "_".join(dynamic_jsonl_suffix_parts) if dynamic_jsonl_suffix_parts else "default"

    base_output_dir = project_root / "outputs" # Assume outputs folder at project root

    if args.chunks_path is None:
        args.chunks_path = base_output_dir / Path(f"phase_a_processed_chunks_{dynamic_jsonl_suffix}.jsonl")
    if args.parents_path is None:
        args.parents_path = base_output_dir / Path(f"phase_a_processed_parents_{dynamic_jsonl_suffix}.jsonl")
    
    if not args.chunks_path.exists():
        logger.warning(f"Dynamically determined chunks-path '{args.chunks_path}' does not exist. Please run Phase A first with matching suffixes.")
        sys.exit(1)
    if not args.parents_path.exists(): # Parents path might be empty if no parent_document strategy was used, this is acceptable.
        logger.info(f"Dynamically determined parents-path '{args.parents_path}' does not exist. Proceeding without parent documents for storage.")


    # --- Dynamic Vector Store persistence path construction (now covers both Chroma and FAISS) ---
    dynamic_persist_sub_path_parts = []
    dynamic_persist_sub_path_parts.append(model_slug(args.output_suffix_chunking_strategy))
    if args.output_suffix_doc_type:
        dynamic_persist_sub_path_parts.append(model_slug(args.output_suffix_doc_type))
    dynamic_persist_sub_path_parts.append(model_slug(embedder_provider))
    dynamic_persist_sub_path_parts.append(model_slug(embedding_model))
    
    dynamic_persist_sub_path_name = "_".join(part for part in dynamic_persist_sub_path_parts if part)
    if not dynamic_persist_sub_path_name:
        dynamic_persist_sub_path_name = "default_rag_index"

    # Use the generic new arg for base path, which can be configured for Chroma or FAISS.
    final_vector_store_persist_path = project_root / args.vector_store_base_path / dynamic_persist_sub_path_name
    
    logger.info(f"Vector Store persistence path: {final_vector_store_persist_path}")
    logger.info(f"Child chunks collection: '{args.child_collection_name}'")
    logger.info(f"Parent documents collection: '{args.parent_collection_name}'")

    # Initialize Embedder for the query (same as used for embedding chunks)
    # This requires access to your get_embeddings function and settings.
    # You might consider packaging get_embeddings as a utility function.
    # 1. Initialize Embedder (validate by dummy call)
    try:
        # We need a callable that matches the `get_embeddings` signature for the batch loop.
        # This lambda effectively 'binds' the settings and config to the get_embeddings function.
        embedder_callable = lambda texts_to_embed: get_embeddings(
            embedder=embedder_provider,
            texts=texts_to_embed,
            model=embedding_model,
            settings=settings,
            rag_config=rag_config
        )
        
        # Validate embedder setup by trying a dummy call
        _ = embedder_callable(["test text for embedder initialization"])
        logger.info(f"Embedder '{embedder_provider}' for model '{embedding_model}' initialized successfully for testing.")
    except SystemExit as e:
        logger.error(f"Failed to initialize embedder: {e}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"An unexpected error occurred during embedder initialization: {e}", exc_info=True)
        sys.exit(1)


   # 2. Initialize Vector Store Collections
    vector_store_class: Type[BaseVectorStore] = VECTOR_STORE_REGISTRY[selected_vector_store_provider]
    
    # Common config for the VectorStore instance
    common_store_config = {
        "persist_directory": final_vector_store_persist_path, # Base path for persistent data
        "overwrite": args.overwrite,
    }

    # Add provider-specific config parts
    if selected_vector_store_provider == "chroma":
        common_store_config.update({
            "mode": args.chroma_mode,
            "host": args.chroma_host,
            "port": args.chroma_port,
        })
        # Collection-level metadata for Chroma, for filtering or inspection
        chroma_hnsw_space = rag_config.get("chroma", {}).get("hnsw_space", "cosine")
        child_collection_metadata = {"hnsw:space": chroma_hnsw_space, "embedder": embedder_provider, "model": embedding_model}
        parent_collection_metadata = {"collection_type": "parent_documents"} 

    elif selected_vector_store_provider == "faiss":
        faiss_dimension_config = vector_store_global_config.get("faiss", {}).get("embedding_dimension")
        faiss_dimension = args.faiss_dimension or faiss_dimension_config
        
        if not faiss_dimension:
            logger.critical("FAISS requires 'embedding_dimension' in config.yaml (vector_store.faiss.embedding_dimension) or via --faiss-dimension CLI arg.")
            sys.exit(1)
        common_store_config["dimension"] = faiss_dimension
        
        # FAISS does not have collection-level metadata in the same way Chroma does
        child_collection_metadata = {}
        parent_collection_metadata = {}

    child_chunks_store: BaseVectorStore
    parent_docs_store: BaseVectorStore

    try:
        # Child Chunks Store (embeddable) - FAISS will be configured for actual vector indexing
        child_chunks_store = vector_store_class(
            config={
                **common_store_config,
                "collection_name": args.child_collection_name,
                "collection_metadata": child_collection_metadata, # Passed to Chroma, ignored by FAISS
                "is_vector_indexed": True # Explicitly True for child chunks
            }
        )
        logger.info(f"Vector Store for child chunks '{args.child_collection_name}' is ready. Initial count: {child_chunks_store.count()}.")

        # Parent Documents Store (storage only) - FAISS will be configured without vector indexing (just metadata)
        parent_docs_store = vector_store_class(
            config={
                **common_store_config,
                "collection_name": args.parent_collection_name,
                "collection_metadata": parent_collection_metadata, # Passed to Chroma, ignored by FAISS
                "is_vector_indexed": False # Crucial for FAISS: this store only manages metadata
            }
        )
        logger.info(f"Vector Store for parent documents '{args.parent_collection_name}' is ready. Initial count: {parent_docs_store.count()}.")

    except Exception as e:
        logger.critical(f"Failed to initialize vector store collections: {e}", exc_info=True)
        sys.exit(1)


    # 3. Load Chunks from Phase A Output
    child_raw_dicts = load_jsonl(project_root / args.chunks_path)
    parents_raw_dicts = load_jsonl(project_root / args.parents_path)

    child_chunks_to_embed: List[Chunk] = []
    parent_chunks_to_store: List[Chunk] = []

    for item in child_raw_dicts:
        try:
            chunk_obj = Chunk(**item) 
            # All chunks from the child-chunks-path are intended for embedding in this context
            child_chunks_to_embed.append(chunk_obj)
        except Exception as e:
            logger.error(f"Error deserializing chunk from {args.chunks_path}: {e}")

    for item in parents_raw_dicts:
        try:
            chunk_obj = Chunk(**item) 
            # All chunks from the parents-path are intended for storage as parent documents
            parent_chunks_to_store.append(chunk_obj)
        except Exception as e:
            logger.error(f"Error deserializing parent chunk from {args.parents_path}: {e}")

    logger.info(f"Loaded {len(child_chunks_to_embed)} child chunks for embedding from {args.chunks_path}.")
    logger.info(f"Loaded {len(parent_chunks_to_store)} parent documents for storage from {args.parents_path}.")

    if not child_chunks_to_embed:
        logger.warning("No child chunks found for embedding. Exiting Phase B.")
        sys.exit(0)

    # Phase B Specific Optimization: Only store parent documents that are actually referenced
    # (unchanged from last iteration)
    parent_chunk_ids_referenced_by_children = set()
    for child_chunk in child_chunks_to_embed:
        if child_chunk.metadata and "parent_chunk_id" in child_chunk.metadata:
            parent_chunk_ids_referenced_by_children.add(child_chunk.metadata["parent_chunk_id"])


    parent_chunks_to_store: List[Chunk] = []
    for parent_chunk in parent_chunks_from_file:
        prefixed_parent_id = f"{args.id_prefix}::{parent_chunk.id}" if args.id_prefix else parent_chunk.id
        if prefixed_parent_id in parent_chunk_ids_referenced_by_children:
            parent_chunks_to_store.append(parent_chunk)
        else:
            logger.debug(f"Parent document '{parent_chunk.id}' (prefixed: {prefixed_parent_id}) not referenced by any child chunks. Skipping storage.")

    logger.info(f"Filtered to {len(parent_chunks_to_store)} parent documents that are actually referenced by child chunks.")


     # 4. Embed Child Chunks and Add to Vector Store
    total_embedded = 0
    batches_to_embed = batched(child_chunks_to_embed, args.batch_size)
    for i, batch in enumerate(batches_to_embed):
        texts_to_embed = [c.text for c in batch]
        ids_for_store = [f"{args.id_prefix}::{c.id}" if args.id_prefix else c.id for c in batch]
        
        metadatas_for_store = []
        for c in batch:
            meta_copy = c.metadata.copy()
            if args.id_prefix and "parent_chunk_id" in meta_copy and not meta_copy["parent_chunk_id"].startswith(f"{args.id_prefix}::"):
                meta_copy["parent_chunk_id"] = f"{args.id_prefix}::{meta_copy['parent_chunk_id']}"
            
            meta_copy["pipeline_config_hash"] = overall_rag_pipeline_config_hash 
            metadatas_for_store.append(safe_meta(meta_copy))

        try:
            embeddings_list = embedder_callable(texts_to_embed)
            
            child_chunks_store.add(
                ids=ids_for_store,
                embeddings=embeddings_list,
                documents=texts_to_embed,
                metadatas=metadatas_for_store
            )
            total_embedded += len(batch)
            logger.info(f"Embedded batch {i+1}/{len(batches_to_embed)} (Total: {total_embedded} child chunks).")
        except Exception as e:
            logger.error(f"Error embedding batch {i+1}: {e}. Skipping batch.", exc_info=True)
            
        if args.sleep > 0:
            time.sleep(args.sleep)

    total_added_count = child_chunks_store.count()
    logger.info(f"Successfully added {total_added_count} child chunks into vector store '{args.child_collection_name}'.")

    # 5. Store Parent Documents in a separate Vector Store (without embeddings)
    if parent_chunks_to_store:
        parent_ids_for_store = [f"{args.id_prefix}::{c.id}" if args.id_prefix else c.id for c in parent_chunks_to_store]
        parent_texts = [c.text for c in parent_chunks_to_store]
        
        parent_metadatas_for_store = []
        for c in parent_chunks_to_store:
            meta_copy = c.metadata.copy()
            meta_copy["pipeline_config_hash"] = overall_rag_pipeline_config_hash
            parent_metadatas_for_store.append(safe_meta(meta_copy))

        try:
            # For Chroma: embeddings=None will work fine for a metadata-only collection.
            # For FAISS (is_vector_indexed=False): its add method is designed to just append to metadata JSONL.
            parent_docs_store.add(
                ids=parent_ids_for_store,
                embeddings=None, # Explicitly None for parent store (no embeddings in this collection)
                documents=parent_texts,
                metadatas=parent_metadatas_for_store
            )
            logger.info(f"Successfully stored {len(parent_chunks_to_store)} parent documents into vector store '{args.parent_collection_name}'.")
        except Exception as e:
            logger.error(f"Error storing parent documents in vector store: {e}", exc_info=True)
    else:
        logger.info("No parent documents to store.")

    # Persist the vector store data â€“ especially crucial for FAISS
    child_chunks_store.persist()
    parent_docs_store.persist()

    logger.info("Phase B complete: Vector Store populated with child chunks and parent documents.")


if __name__ == "__main__":
    main()